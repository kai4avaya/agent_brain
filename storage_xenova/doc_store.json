{"docstore/metadata":{"dummy":{"docHash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="},"2401.05391.pdf||FPgfDWs0WtFnGL7lRuconY+HIB0RmbLjVaE5+fEX4Vo=":{"docHash":"HmA2XwVTR+CHBAKqJqQ2wXPbdtw2AT3fSK2DDgDcITA="},"2401.05391.pdf||SoBeMvGB1QubAbrhzjQTWp4hsrXKG4nwbTrR0lVEZaA=":{"docHash":"7xFNBhKAYZa1XOyD97YQC0A8XxcoDtbQ+4lsILNiolo="},"2401.05391.pdf||CkgjS4b8v+90JCWIwF0XjwhzUgTy4MwSwMgHUMAdLi8=":{"docHash":"y7wq79nMrTiq6rbWg1t9j0I1+OY8wAHJsOImrVpxMtQ="},"2401.05391.pdf||PHMAujqg3eZ1PkPWK5PE7YXBgvpkcXYWt5aALI3o29s=":{"docHash":"wg+dYSAIDLG2kHzuKcYiDG006wdKetfif4UKJW57HKo="},"2401.05391.pdf||0XBRAARuuqBKBmmcn4NR6XEfLtHhFWNuIN8pMZ1Um4I=":{"docHash":"bZ4ZOdNjFNPMT3V2+rLlB2dZVj6eCGRTRxwZOz1rEcg="},"2401.05391.pdf||fhS37/t0afYTMesAWz1Rzdi5ZxlQH3pn1h2bx25uMso=":{"docHash":"rQxtsqILNEQC3pyKsvCgv3F1HQS2dO7XqhgWe324vQM="},"2401.05391.pdf||68yz/qH1CKcYhL0Gm7Lnh1vl7yLjRfi5tkA+I03QvYU=":{"docHash":"lWPBxhZU2ezjIaFjoLmhPqz4TPV4cMuh6cVnyQXy6HQ="},"2401.05391.pdf||tnNg4cV+xlEYfl/0P/oKvIptKfdU+5VeXX/aLGeYbt8=":{"docHash":"gXSFARthyxMlnuji4dA6neUXgHj7xKvpw5YBTFWXh2I="},"2401.05391.pdf||cuua4OU7/eRcqfCw+feXEHT7a3fxuFdex8NeWk/RVLI=":{"docHash":"DCtkBGXbKSEiPGrUW/EiG1DzOD0PfGk1GdUYT1s9BuU="},"2401.05391.pdf||ahinDG5CS6JNHc0LykVHln7mmqxlEAJc4oBU80Y6RUs=":{"docHash":"6U50nk7qOiZBbufUlwVfRIFLdiS03xQu58PrKbbHSIA="},"2401.05391.pdf||kdLwvQdHeAvOXGhqdT/Jhep7xdgouLZHk1xSV6y76Lo=":{"docHash":"CYBy+JOR7pHIbqVPsUnt7i9IvWQZHRRR5WFm8MNqvmE="},"2401.05391.pdf||ZgqiqrnqxytWGHAvVDkRtCJAufj7OM6Dq65qoWqaiM0=":{"docHash":"v5PIz6oDJZPigg3WR25t0Xd7vFh5EZINsfDVk4t8g4s="},"2401.05391.pdf||N96CZB5UOzfpjjxBj/pX8sh6diAYTORnU5FhZ98VOug=":{"docHash":"/xPe2sfcb+B+BovAL4SCME6k6imUcbEFFVauLWheWnU="},"2401.05391.pdf||PN27Am8tNMOXdDexl0RPsb7mp3lm16abkKdNgQRC11g=":{"docHash":"roIvSBqUE4Vwp/gh04EjAOuMN1BOklwoA1MGAN7tBuo="},"2401.05391.pdf||AFYEcw7idVLwi4JQtpVAhUEzH8E+vAqkNqT0u+IjbQ0=":{"docHash":"WENLqrRRO4Tr8x5Oan/0ag6yYcj+++9qw4/7vvtJtyg="},"2401.05391.pdf||frcCV1k9oG9oKj3dpUqdJg1PxRT2RSN/XKdLCPjaYaY=":{"docHash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="},"corporate_homedepot_com_.txt||14kna":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"essay.pdf||4YcMzZLR/BL09BruoFDGPRA2vb8xcRXtFvZtKsIVjDA=":{"docHash":"1UoYTTYohqdtk4DkVFCjTZm0soevXTvAE5WnseNPYPw="},"essay.pdf||mg591T7N8x8xBVVfOu26Q6GDKhdlU032K6IvJVBAZvg=":{"docHash":"X6rscDaCU/pNL/GGb0gNPFRJHemM1g+NKvow1YLHJYo="},"essay.pdf||j+D09K2bvXXJsI4Lyr2pTE7NhdSvCTr5oiijxwDd4GQ=":{"docHash":"vAAj9Z/qxMAgzrX+lRtWqxzEhx0iXRzcrwMhUakG0Fo="},"essay.pdf||8uO5RtoMt6K1rDdWpLDf1J3FyomO4y6vYAr8W8gwAcM=":{"docHash":"2Agysaxlh+Y8l1FVBxJzmhAmpPdKUad5RtBuU/jdqwo="},"essay.pdf||I8IRYbaHIQZkDh0aDwFb3zKPwJ8w9xrzHIMvept79R8=":{"docHash":"ypymveoDsCOjEJLD4tXQJTTWIMiQ63xoEzH3A98F6Vs="},"essay.pdf||+dKnlkpfisOY1lBGC3O0heom5/B3/34cC7OEUM1p9NM=":{"docHash":"ddsVeiVLgvoWVZmAPbb1Yxk94RqYQ8lxmlSHk2U7+ig="},"essay.pdf||Z/fmghpU+nLad+Ro9mun6AQ4k0P8egZ4pgIcq6TYgPE=":{"docHash":"AcvYCu9SXnlz92d0Jah08uEfukk5OWA3DsNccB4dwo8="},"essay.pdf||QukymsQibVbJyCoYH6SuN55rjl/sGCTwHIZfBIKrtjs=":{"docHash":"ZCoTcwFFf6IVETEeE5+l9zg2dFhM+zZa3VAGIngbBmk="},"essay.pdf||/QanAuJ+7y0ZjTptqVeO8pbeXFT2gpW6Ob/LOoZSvO0=":{"docHash":"q5z96uNwEHIfujpRKaOGstNlKwry6iEF82sfB7W4PEo="},"essay.pdf||pEUEspFmpXDtZzpPZCWHQYbgoI0q+cJT4La12LHuhzI=":{"docHash":"vshsGLKCwT6drgZWkpCAOHl5QLThz5baV+XDjTdh8tI="},"essay.pdf||FeyRGTbuLhwCOCYxXZhHpwd6nuub/T10TFTNHIDJin8=":{"docHash":"MS9vSCE4AjINrFu/a93t4YV5UolISLSfE1I5Oj7aWVE="},"essay.pdf||/jkjEGY6UxdUsTMGTf7YglD20FCJfpQanlKHMiuy6cY=":{"docHash":"FBn9Yr11OXl3mwmsL3yz/g+jIV7HPXXKCIrg2TtW9/c="},"essay.pdf||/y2IZVS31txDAjm3H617Sdpz14hB8T72qfsienrU4ek=":{"docHash":"0l3uW0FjqmcaRmkteU6bJEAQbbNBfI0Kx59MyJAJdjU="},"essay.pdf||QcklhPs33xTIMyeacu01w9LS+l+u2+HUTaN6awrz+BQ=":{"docHash":"cgIo+rJncSbrjVng/b827dZfNjaxZgM62OYDtqOokyM="},"essay.pdf||wn/J2PyaEjVQX7jkicfo6si2JGGn+FLGyrcwaN9ORiA=":{"docHash":"PGir8rdNOIAx6nx/6CElwT7LyEhULrH+7Kfuk27jlCQ="},"essay.pdf||mTqQb2yceI3Otc7/I2+229A97vK67/xknKuvavo8p88=":{"docHash":"bYoqbLUJiZ/tcayt4PjvFm6TX+iab/yLuNx8gqsdPk4="},"essay.pdf||whEERXPHbiOjJRpOIBm8HPTkHYYZJ/cqZYpoe/qrhe8=":{"docHash":"yvL8g3YcQuNsWUqjXHxvOrC/g1LuNc+HEyPNPbbaDQ0="},"essay.pdf||Ft6v52B2K3Z3zf0d+7UenLQpXXF6jTjkl2jqGbOWhlc=":{"docHash":"/jKt1cBEY99awXmLeroZ9vtTe0ig3LQQdlFbTvu0Szc="},"essay.pdf||AVro3p5zYiuJ0QXPJftCAZR8m/dK9j8Rn4N177fY6mU=":{"docHash":"izfVOoHh1Ugg3+5mn8ZcvM+xsgS0nwcF0U3Wl6NzujE="},"essay.pdf||98yff55I1Orl0Nxh93nipnrNbr6WMtxKHXc1eOwZi/s=":{"docHash":"q3M0Qn5wfsqoFLeoFqtowC3jGXGgkHx0kauwoIXsYR0="},"essay.pdf||3RkIAqyXKUOIVxbg53LAYPx1OAlclGPJWcV0tmfBPF8=":{"docHash":"rss+Cpt0MvClJTdB3EsaYcOS5CArTbZyJhZyI0ajO80="},"essay.pdf||frcCV1k9oG9oKj3dpUqdJg1PxRT2RSN/XKdLCPjaYaY=":{"docHash":"E4DK+l2P7nvw5uqGFI76nawW8ADkXSvaL72FFLQSAZs="},"price_matching (2).docx||shgnz":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||n7r6q":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"67999390-1e05-40e0-91c1-3c4562891a6f":{"docHash":"hueMc9fTj3RirbOvBysZRm68M674usNkUfO4dOP0nMs=","refDocId":"2401.05391.pdf||FPgfDWs0WtFnGL7lRuconY+HIB0RmbLjVaE5+fEX4Vo="},"c2c49fac-2af2-4f46-9532-31d7e0b237c5":{"docHash":"g9VpuREacIgsLju6oXjzQKrau7ynf2Q0p4Zo/1ZeGqE=","refDocId":"2401.05391.pdf||SoBeMvGB1QubAbrhzjQTWp4hsrXKG4nwbTrR0lVEZaA="},"44d6fc32-366c-41fe-a557-c9f6fba4592e":{"docHash":"fCtNlg294zxBjJgGOdJic+VG4ecgP4V+v//w7hYV2Nc=","refDocId":"2401.05391.pdf||CkgjS4b8v+90JCWIwF0XjwhzUgTy4MwSwMgHUMAdLi8="},"56dbac1d-3be7-40cc-b8c4-14279a1ef278":{"docHash":"VFu43mQ+dsZ2DSEWZ+GmtPn8/bZaIKkUy/P+zGNYpZs=","refDocId":"2401.05391.pdf||PHMAujqg3eZ1PkPWK5PE7YXBgvpkcXYWt5aALI3o29s="},"8cf778f9-1718-4805-8ba8-d805ac2ca2f9":{"docHash":"BYcvz15iZAKDpDFCzkb8Etdj9xHYXrpN5gwObwV6Drs=","refDocId":"2401.05391.pdf||0XBRAARuuqBKBmmcn4NR6XEfLtHhFWNuIN8pMZ1Um4I="},"61741d4c-5a5d-4532-8a47-7840b5f875d7":{"docHash":"XUPkkZONsmYoJV3VhwILH4YHkmZkB3k30civEU9mnfc=","refDocId":"2401.05391.pdf||fhS37/t0afYTMesAWz1Rzdi5ZxlQH3pn1h2bx25uMso="},"07fc4094-f845-422a-883c-1f218646e43e":{"docHash":"3djKWgRe7/626hjgITUKPQ84bIZhqhv5xsa/xtOcBdM=","refDocId":"2401.05391.pdf||68yz/qH1CKcYhL0Gm7Lnh1vl7yLjRfi5tkA+I03QvYU="},"ab43e7fc-73bf-4a9e-88ea-b5bdbcd67879":{"docHash":"DbdQPK2FbFQjAFEUEc3HFH9zrYAEDTObrTSlmBLRuLk=","refDocId":"2401.05391.pdf||tnNg4cV+xlEYfl/0P/oKvIptKfdU+5VeXX/aLGeYbt8="},"75d1f433-1cdc-4b7f-a696-1e8c07ece9f7":{"docHash":"BCmoC7YlRztypgiCtlcqnzMI7gyInl4bh4YHVjL86aQ=","refDocId":"2401.05391.pdf||cuua4OU7/eRcqfCw+feXEHT7a3fxuFdex8NeWk/RVLI="},"a7b0b44d-b90b-4c8e-8b3e-f73aa39010d7":{"docHash":"MDiWd97gQ1ntsec9K+CbhWWzTv1f+4vsNwj2mANwBUs=","refDocId":"2401.05391.pdf||ahinDG5CS6JNHc0LykVHln7mmqxlEAJc4oBU80Y6RUs="},"a2d69a4b-410e-41e7-803f-3cae26b021c2":{"docHash":"XwTiAKrse+xmFI8aXKn7pDbmo/jMnXF9/r91DWY4N6w=","refDocId":"2401.05391.pdf||kdLwvQdHeAvOXGhqdT/Jhep7xdgouLZHk1xSV6y76Lo="},"cc0cc649-f4b0-4459-ba43-282aa91db279":{"docHash":"pZ/3vK7i8toP+dRhrh1zbjyptqh9SsSwU76aosqBusM=","refDocId":"2401.05391.pdf||ZgqiqrnqxytWGHAvVDkRtCJAufj7OM6Dq65qoWqaiM0="},"291847c9-5083-4aed-9446-baafe0c7935a":{"docHash":"nsilPdtYydWzdZfqj3fgdi+g9C55RIVxjAiTi33l3go=","refDocId":"2401.05391.pdf||N96CZB5UOzfpjjxBj/pX8sh6diAYTORnU5FhZ98VOug="},"1db7a1aa-9845-44c2-bdf7-7725be1ef807":{"docHash":"faGMJ8lgqkal6fczmqkec2o59NsMtbXi8yZVSeo3h3I=","refDocId":"2401.05391.pdf||PN27Am8tNMOXdDexl0RPsb7mp3lm16abkKdNgQRC11g="},"4ff7662b-9440-4d99-95fb-f3fcb7839d30":{"docHash":"YmVKLjKcpeVVKepAkvXlf3GJgK6h29/TGH9PyjvLuaU=","refDocId":"2401.05391.pdf||PN27Am8tNMOXdDexl0RPsb7mp3lm16abkKdNgQRC11g="},"2c367b1d-3edd-4de6-bdab-c56d0e114216":{"docHash":"BvFtGE1XcFnGnROh2aZ19i+ZoWOiBDkEcF9kyNBuZ0c=","refDocId":"2401.05391.pdf||AFYEcw7idVLwi4JQtpVAhUEzH8E+vAqkNqT0u+IjbQ0="},"6ff95514-ceb9-4d8e-8b45-4bf7abc14b83":{"docHash":"oVFJRgfUu0b97tw24FeDKWWzxPAdHabjXwP6HMN5is0=","refDocId":"2401.05391.pdf||AFYEcw7idVLwi4JQtpVAhUEzH8E+vAqkNqT0u+IjbQ0="},"99f6f9a7-46f0-4c3a-a80d-7480f937b103":{"docHash":"E+CrBYCmHFrmkQJOrs91WMMpddQzr9cyh+BfhmVaMgU=","refDocId":"corporate_homedepot_com_.txt||14kna"},"78730d49-754b-4cc7-96a4-fa3192cb694c":{"docHash":"PelEE+YkRcP6ATTylgcaZ/7iEjuEJzws22L9dapQ/IQ=","refDocId":"corporate_homedepot_com_.txt||14kna"},"81155eae-fc19-4ab0-bd03-531194a5ecde":{"docHash":"xlS3BgjAApHX+sIhrghm85pWtrUauCWXQf/g+HCGW3k=","refDocId":"essay.pdf||4YcMzZLR/BL09BruoFDGPRA2vb8xcRXtFvZtKsIVjDA="},"6c5773ed-a2cd-4293-ae91-92813a73a125":{"docHash":"m+J3CjFsmQ8As3Fb1VVa/6KtHucw1fJ8Plct5490Jqw=","refDocId":"essay.pdf||mg591T7N8x8xBVVfOu26Q6GDKhdlU032K6IvJVBAZvg="},"1497e329-c523-4308-b5cd-c2f04631cfba":{"docHash":"A0rKv5I09+AxVl3t8XxEdIm/tWjIpotRVkjSmOMN56c=","refDocId":"essay.pdf||j+D09K2bvXXJsI4Lyr2pTE7NhdSvCTr5oiijxwDd4GQ="},"67597179-519d-495c-9849-f3777fcff97f":{"docHash":"xluzNpGbY2zBi4dqm6kY+6w5BtYyYoZadmGn9pvmQ6w=","refDocId":"essay.pdf||8uO5RtoMt6K1rDdWpLDf1J3FyomO4y6vYAr8W8gwAcM="},"ae1e3cdc-9780-4e35-bc5e-289c158a5026":{"docHash":"AH+UeyTPPzKrJNSkLTPtvhElC2GNoF/vrsk/6EPU/Gs=","refDocId":"essay.pdf||I8IRYbaHIQZkDh0aDwFb3zKPwJ8w9xrzHIMvept79R8="},"1322b5d4-acd3-4865-a45a-c8074862f3dd":{"docHash":"ko7pXKs13Nfs/bwpDehIImw5H/TyZDcFCp47uzkUn6U=","refDocId":"essay.pdf||+dKnlkpfisOY1lBGC3O0heom5/B3/34cC7OEUM1p9NM="},"16150cba-cff9-49c2-9ba0-3390bd9ce96f":{"docHash":"QMcd9lXFvcVwa055RRhiJad/dOmU4ipYq0radpo9yw0=","refDocId":"essay.pdf||Z/fmghpU+nLad+Ro9mun6AQ4k0P8egZ4pgIcq6TYgPE="},"a0aa49aa-c043-4c19-88da-792eda43c5a7":{"docHash":"i3Qx9xRTnYGIxfWqxDYWROdKgUA0N/1FHMYk/6t47+c=","refDocId":"essay.pdf||QukymsQibVbJyCoYH6SuN55rjl/sGCTwHIZfBIKrtjs="},"f8d24ac5-62a9-4a08-b325-d67ba54fa5c2":{"docHash":"fjLSUAsH/EoZ63lWNKyiAbgWiAxzJBhRW0b5hxMN2TE=","refDocId":"essay.pdf||/QanAuJ+7y0ZjTptqVeO8pbeXFT2gpW6Ob/LOoZSvO0="},"b6d30409-8c58-4d0f-a767-dcb90d312c83":{"docHash":"I4rCiu2j2yCgUx/FP4QVk0X/F8GxudiI8B8wVGJ5ibo=","refDocId":"essay.pdf||pEUEspFmpXDtZzpPZCWHQYbgoI0q+cJT4La12LHuhzI="},"44ebb065-39b9-4308-ada5-32eacc0e96fe":{"docHash":"ul9sHNNOZRYnrX9Blu+VIIHbw1hPV3e5wtc7HvBs3Pk=","refDocId":"essay.pdf||FeyRGTbuLhwCOCYxXZhHpwd6nuub/T10TFTNHIDJin8="},"0e1f51a7-8e39-41be-97ee-3a5ac4097a21":{"docHash":"h0IQ+ECvLHEWr3v8B5cODaNu/t2f0RqyLJ1wp/3JTAY=","refDocId":"essay.pdf||/jkjEGY6UxdUsTMGTf7YglD20FCJfpQanlKHMiuy6cY="},"47c37987-d689-4f7d-af71-50812fd50231":{"docHash":"c3HCiXeig7xT2gZVXryTaGCwoFusZrzH/u4DrsVm+lQ=","refDocId":"essay.pdf||/y2IZVS31txDAjm3H617Sdpz14hB8T72qfsienrU4ek="},"e68e1428-f18e-4d46-a272-ea7a1db0856f":{"docHash":"aG/WPJ0VD9RJYlt1dt+D3/Ylps6F58aqICnnlY4hQ/o=","refDocId":"essay.pdf||QcklhPs33xTIMyeacu01w9LS+l+u2+HUTaN6awrz+BQ="},"aedfc1e7-6307-47af-8373-c9bc476e7ff5":{"docHash":"qis0Ogy2obta7/jKwziruAZIc09DAG8LkJH7uAZVO3c=","refDocId":"essay.pdf||wn/J2PyaEjVQX7jkicfo6si2JGGn+FLGyrcwaN9ORiA="},"28b4a213-6372-4af3-a71e-5704193001c6":{"docHash":"7dJlQmecBVuGlm3Q4IHgfQEg491AMY7SB+PLH2e1KEs=","refDocId":"essay.pdf||mTqQb2yceI3Otc7/I2+229A97vK67/xknKuvavo8p88="},"cd677ed3-a582-4887-baaf-c067a62efcef":{"docHash":"fmQKx1BgAHzG+dvwqSFkk9NitNFXe6WjC2goJCk/MEk=","refDocId":"essay.pdf||whEERXPHbiOjJRpOIBm8HPTkHYYZJ/cqZYpoe/qrhe8="},"f19f7184-9095-4b01-a643-2dee85348df0":{"docHash":"WEHy3NpRBfRUCjwp126fIw/usFVAmefpavTjsv+9CN0=","refDocId":"essay.pdf||Ft6v52B2K3Z3zf0d+7UenLQpXXF6jTjkl2jqGbOWhlc="},"b28e5282-d618-4e48-b479-15effa69bc03":{"docHash":"x/c+ZBjq7IIxWEO63YoOGcUxj1VHo2miwW9CsUI61Gs=","refDocId":"essay.pdf||AVro3p5zYiuJ0QXPJftCAZR8m/dK9j8Rn4N177fY6mU="},"ac8627d4-3f97-4f0c-acce-9fd6a56da1fe":{"docHash":"SjSlwLEb4sEp+E4izN26GAop3SAaQTI6Z7OuD/c3d7I=","refDocId":"essay.pdf||98yff55I1Orl0Nxh93nipnrNbr6WMtxKHXc1eOwZi/s="},"fac9fb71-abdd-40b5-99b6-ab1d255fc8a7":{"docHash":"Pqdg2Q72YpnO4O+0DwvBd7ojdD4CtztYgBb/dnXCERw=","refDocId":"essay.pdf||3RkIAqyXKUOIVxbg53LAYPx1OAlclGPJWcV0tmfBPF8="},"9dafdb42-4539-487d-b0dd-8ce49f8761a4":{"docHash":"fdRkmmXqZfSb9q6O0+ln/zbDiYlv61YoT3rSdz9lWsM=","refDocId":"price_matching (2).docx||shgnz"},"85bdc115-15b9-4d13-a4bb-7173d7061fb5":{"docHash":"icjMNTbB8GynO6TdniFHTNgETrodsK03wJTIDU4H+Do=","refDocId":"www_homedepot_com_c_customer_service.txt||n7r6q"},"bfe92bca-1fdb-44c5-92ad-c8bfd3eccd2a":{"docHash":"i9AVusQwwsnVFevBJPx+w3dvm3PloTc+ZDBTOO2R1+8=","refDocId":"www_homedepot_com_c_customer_service.txt||n7r6q"},"corporate_homedepot_com_.txt||ywu9w":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||tdse9":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||9d830":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||s41o7":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||f7bzm":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||fvo3z":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||4cmq6":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||0o5re":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||mjq4v":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||tnf1n":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||48olp":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||aa8tp":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||xne92":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||tjo02":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||xog1m":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||ygxz5":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||g6v6h":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||wo1lj":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||ex649":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||t8rvu":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||3scxg":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||2i3s7":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||dm06i":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||rxwu4":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||wz6r5":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||gkanh":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||uwhcl":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||37qkp":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||trxow":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||c21iy":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||rc6bf":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||tta0g":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||24efh":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||246px":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||9lcju":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||wzvpv":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||z0olm":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||fdoqk":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||ndi8o":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||h0dgz":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||mci4w":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||e6pae":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||cror2":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||4a61d":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||br5f5":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||sotdt":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||zw53i":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||bgzd5":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||uuc28":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||fnljv":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||3dzp1":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||yc83t":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||er2e1":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||vxsrl":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||s9xr7":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||mhlhp":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||9kehu":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||6gtp8":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||3t3vo":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||owvoi":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||ya528":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||cdr2s":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||5q1la":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||3ef2f":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||sphj2":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||fa5vb":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||gpymy":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||zfaot":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||0d6uz":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||7zie0":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||wyxeg":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||ymvv1":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||azsmy":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||o8hqb":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||ypzh5":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||lnv4p":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||ua39g":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||2l0rn":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||bduq6":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||n7opl":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||h1xmw":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||ppwp9":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||5z63s":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||9tked":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||8g74h":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||evbpt":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||qto7x":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||nnglm":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||pf69s":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||h6gk3":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||t5xgw":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||np8gt":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||y36bp":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||azn4c":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||jlxz5":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||d1ji2":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||r9e37":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||rkdym":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||fdoa0":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||y7s8f":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||j0kh3":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||5aevz":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||bzl72":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||3f444":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||5t2jo":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||z4lbc":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||5npw6":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||dzgzh":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||g9jhp":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||cfwlm":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||505hx":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||nyojr":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||5h370":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||aqj2j":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||qasxa":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||0wos9":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||4h2gd":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||dgnd9":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||d2vwt":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||9gyru":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||aqn5q":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||pqjyz":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||uytvt":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||par4k":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||546ka":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||gfmze":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||3bb2u":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||v6e3z":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||dkk02":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||0dzt7":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||bqxsu":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||edy0x":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||o7ivd":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||ytywe":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||6rdmu":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||fuaor":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||nkw6f":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||jh9mj":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||mp1fs":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||auskw":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||sp21h":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||kcux5":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||4rmpb":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||lk4rb":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||869sh":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||3l0uv":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||oqism":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||pd4pg":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||ktcqf":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||4didv":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||xlpnn":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||p31x7":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||l775j":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||mfzww":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||muuok":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||v50el":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||gujzy":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||eqs95":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||9m518":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||8qyt0":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||nas1x":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||wcz0k":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||1f8jw":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||kqsb0":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||6dlhj":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||qvr2f":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||81igh":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||esqux":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||5qv3h":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||m417z":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||lh1n7":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="},"corporate_homedepot_com_.txt||p8qfk":{"docHash":"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY="},"price_matching (2).docx||ye3vv":{"docHash":"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg="},"www_homedepot_com_c_customer_service.txt||3kahs":{"docHash":"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs="}},"docstore/data":{"67999390-1e05-40e0-91c1-3c4562891a6f":{"__data__":"{\"id_\":\"67999390-1e05-40e0-91c1-3c4562891a6f\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"2401.05391.pdf||FPgfDWs0WtFnGL7lRuconY+HIB0RmbLjVaE5+fEX4Vo=\",\"metadata\":{},\"hash\":\"HmA2XwVTR+CHBAKqJqQ2wXPbdtw2AT3fSK2DDgDcITA=\"}},\"hash\":\"hueMc9fTj3RirbOvBysZRm68M674usNkUfO4dOP0nMs=\",\"text\":\"Efficient LLM inference solution on Intel GPU \\r\\nHui Wu,  Yi Gan,  Feng Yuan,  Jing Ma,  Wei Zhu,  Yutao Xu,  \\r\\nHong Zhu,  Yuhua Zhu,  Xiaoli Liu,  Jinghui Gu \\r\\nIntel® Corporation \\r\\n{hui.h.wu, yi.gan, feng1.yuan, jing1.ma, wei2.zhu, hong.zhu, yuhua.zhu, xiaoli.liu, \\r\\njinghui.gu}@intel.com \\r\\n \\r\\nAbstract \\r\\n    Transformer based Large  Language  Models  (LLMs) have been  widely  used  in  many fields, and the \\r\\nefficiency of LLM  inference becomes  hot  topic in  real  applications. However, LLMs  are  usually \\r\\ncomplicatedly  designed in  model  structure  with  massive operations and perform  inference in  the auto-\\r\\nregressive mode, making it a challenging task to design a system with high efficiency. In this paper, we propose an efficient LLM inference solution with low latency and high throughput. Firstly,  we  simplify  the  LLM  decoder layer  by fusing data  movement and  element-wise operations to \\r\\nreduce the memory access frequency and lower system latency. We also propose a segment KV  cache \\r\\npolicy  to keep  key/value  of the request and  response  tokens  in  separate physical  memory for effective \\r\\ndevice memory management, helping enlarge the runtime batch size and improve system throughput. A \\r\\ncustomized  Scaled-Dot-Product-Attention kernel  is  designed  to  match our  fusion  policy  based  on  the \\r\\nsegment KV cache solution. We implement our LLM inference solution on Intel® GPU and publish it \\r\\npublicly. Compared with the standard HuggingFace implementation, the proposed solution achieves up to \\r\\n7x lower token latency and 27x higher throughput for some popular LLMs on Intel® GPU. 1. Introduction    \\r\\nRecently, Transformers [1] based Large Language Models (LLMs) [2-4] have received widespread \\r\\nattention. With the capability of content understanding and generation, LLMs have been applied in many \\r\\ndownstream applications [5-9]. However, LLMs are usually complicated designed, growing larger [10] \\r\\nand deeper [11], making it a challenging task to achieve a highly efficient LLM inference system. LLM inference systems are usually used in  different  scenarios like the latency-critical online  and \\r\\nthroughput-oriented offline  applications [12,  13]. For  online serving,  the  latency  indicates  the  time \\r\\nconsumption of generating some tokens, reflecting the online system fluency, the lower latency the better \\r\\nuser experience. For offline application, the throughput indicates the number of tokens generated at a time, \\r\\nreflecting the system resource utilization ratio, the higher value of the throughput the lower cost of the \\r\\nsystem. For online mode, it  is  critical  to  lower  the latency and improve  the response  efficiency for request. However, the deep  decoder layers combined  with multiple operations make it  not  easy to  achieve  low \\r\\nlatency  for  a  single  token. Taking  Llama2 [14] for  example, the  basic structure is  shown  in  Figure  1, \\r\\ncontaining multiple decoder layers with several modules in each, like Linear, Rotary Position Embedding \\r\\n(RoPE), Scaled Dot Product Attention (SDPA), Root Mean Square Layer Normalization (RMSNorm) and \\r\\nActivation. Simplifying the model structure and reducing the number of operations in LLM structure can \\r\\nhelp lower the latency. Throughput criteria should  also  be considered to  reduce  cost. Enlarging  the  batch  size  value  and \\r\\nimproving the occupancy of computation resource can help achieve high throughput. However, the auto-\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"c2c49fac-2af2-4f46-9532-31d7e0b237c5":{"__data__":"{\"id_\":\"c2c49fac-2af2-4f46-9532-31d7e0b237c5\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"2401.05391.pdf||SoBeMvGB1QubAbrhzjQTWp4hsrXKG4nwbTrR0lVEZaA=\",\"metadata\":{},\"hash\":\"7xFNBhKAYZa1XOyD97YQC0A8XxcoDtbQ+4lsILNiolo=\"}},\"hash\":\"g9VpuREacIgsLju6oXjzQKrau7ynf2Q0p4Zo/1ZeGqE=\",\"text\":\"regressive principle makes LLM inference a memory consuming system, limiting the batch size value and \\r\\nthe throughput that can be achieved on HPC platform with constraint device memory. Generally, during \\r\\nLLM inference process, the input tokens (prompt) will be computed at first in prefill phase and the output \\r\\ntokens (response) will be generated step by step in decoding phase based on the previous generated tokens \\r\\nuntil the terminated token being achieved. At each time step in the decoding phase, some candidate tokens \\r\\nwill be generated at a time, and beam search method [15] is usually applied to select some tokens with \\r\\nhigh confidence scores. With this auto-regressive principle, the key/value of each decoder layer at each \\r\\ntime step are kept on device memory during the whole decoding phase (named as KV cache policy [13]), \\r\\nconsuming more and more memory with time step increasing. It is critical to effectively manage the device \\r\\nmemory to enlarge the batch size and improve the throughput. In this paper, we design and implement a LLM inference solution with low latency and high throughput \\r\\non Intel® GPU, currently covering some of the most popular LLMs including GPT-J [16], Llama [4] / \\r\\nLlama2 [14], OPT [18], and Bloom [19]. The main contributions of this paper include: \\r\\n1. We propose an  efficient  LLM  inference solution and  implement  it on Intel® GPU. The \\r\\nimplementation is available on-line with our Intel®-Extension-for-Pytorch repository. 2. To lower latency, we simplify LLM decoder layer structure to reduce the data movement overhead. In addition, we design a deep fusion policy to fuse both GeMM and Element-wise operations as \\r\\nmore as possible. For some popular LLMs mentioned above with parameter sizes from 6B ~ 176B, \\r\\nour  inference solution achieves up  to 7x lower token latency compared  with the standard \\r\\nHuggingFace implementation (v4.31.0) [20]. 3. To  improve  throughput, we  propose  a segment KV cache policy  to keep prompt  and  response \\r\\nkey/value into discrete device memory, making the prompt key/value being shared by different \\r\\nresponse  tokens to avoid  memory  wasting. On  the  same  device  configuration, the  throughput \\r\\n(tokens per second)  of  our inference solution is up  to 27x higher  than  that  of  the standard \\r\\nHuggingFace implementation for the popular LLMs with parameter sizes from 6B ~ 176B. 4. Based  on our  fusion  policy  and segment  KV cache methodology,  we  design  a highly efficient \\r\\nkernel to fuse  all  computation steps in  SDPA module  together  with possible index  selection \\r\\nprocess for beam search usage. 2. Related Works \\r\\n   The efficiency of Transformers based workloads are usually bottlenecked by memory access [21, 22], \\r\\nwhere  reading  and  writing  data  account  for  a  large  portion  of  runtime. Reducing  memory-bound \\r\\noperations overhead in LLMs can help improve efficiency. Kernel fusion is a general approach to reduce \\r\\nsuch overhead by combining several successive operations together and compute them in a single kernel \\r\\nto avoid frequent memory access to slow GPU high bandwidth memory (HBM). Some previous works \\r\\n[23-26] focused on element-wise operations fusion to reduce kernel launch and memory access overhead. Beyond general fusion on element-wise operations, Fang et al. [26] proposed TurboTransformers to fuse \\r\\nElement-wise and Reduction operations between two GeMMs together. Aminabadi et al. [12] designed \\r\\nsome customized GeMM  kernels based  on  the  characteristics  of Transformer related workloads and \\r\\nadopted a fusion policy to combine operations of data layout conversion and reduction together with the \\r\\ncustomized GeMMs. Dao et al. [21] focused on improving the SDPA module efficiency and proposed a \\r\\nFlashAttention algorithm to combine all computation steps (Batch-GeMM, Softmax, possible Masking, \\r\\nBatch-GeMM) together, achieving up to 3x faster than the standard attention implementation in different\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"44d6fc32-366c-41fe-a557-c9f6fba4592e":{"__data__":"{\"id_\":\"44d6fc32-366c-41fe-a557-c9f6fba4592e\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"2401.05391.pdf||CkgjS4b8v+90JCWIwF0XjwhzUgTy4MwSwMgHUMAdLi8=\",\"metadata\":{},\"hash\":\"y7wq79nMrTiq6rbWg1t9j0I1+OY8wAHJsOImrVpxMtQ=\"}},\"hash\":\"fCtNlg294zxBjJgGOdJic+VG4ecgP4V+v//w7hYV2Nc=\",\"text\":\"user scenarios. Based on FlashAttention [21], FasterTransformer [27] further fused the index selection for \\r\\nbeam search into the SDPA kernel to further remove the data movement operations. Considering LLMs \\r\\nare  weight  bounded  workloads,  compression  techniques  including  pruning  [28],  quantization  [29, 30], \\r\\nknowledge  distillation  [17]  and  low-rank  factorization  [31]  are  usually  applied  to compress the  LLM \\r\\nweight so that to reduce the memory access sizes and improve efficiency. [29, 30] focused on LLM weight \\r\\nquantization and applied low precision Matmul Multiplication for feed-forward and attention projection \\r\\nlayers  in  transformers  to  reduce  the  inference  memory  usage. Consider  irrelevant  information  will  be \\r\\nextracted with long context, [32] adopt sparse transformer to explicitly extract the most relevant segments \\r\\nin attention module to reduce the computation. Many related works also focus on improving the throughput of LLM inference system. Batching is an \\r\\nimportant technique to enhance hardware resource utilization and improve throughput. However, LLM \\r\\ninference  is memory  consuming, limiting  the batch  size  value  can  be set  on hardware  platform  with \\r\\nconstraint device  memory. Effective  memory  management is  critical  to help enlarge  batch  size  and \\r\\nimprove throughput. Due  to  the  auto-regressive  principle,  KV  cache in  LLM consumes  large  device \\r\\nmemory, larger than 30% device memory for a 13B-parameter LLM [33]. To effectively manage the KV \\r\\ncache, Kwon et al. [33] proposed the PagedAttention technique to map logical contiguous key/value to \\r\\nseparate psychical memory, achieving near-zero memory wasting and making flexible KV cache sharing \\r\\nacross different requests. Generally, prompt requests from users are in different sequence length, static \\r\\nbatching (padding different length requests to the max length in a batch) will waste hardware resource \\r\\nsince requests finished earlier cannot be sent back to the client until the whole batch finished. To fix this \\r\\nissue,  ORCA  system  [34]  proposed an iteration-level  scheduling  mechanism  to  invoke  the  execution \\r\\nengine running only a single timestep of the model on the batch so that the finished tokens can be sent \\r\\nback immediately. Moreover, some LLM  inference  engines  like TVM  [35]  and  ONNXRumtime  [24] are  designed  to \\r\\nprovide highly efficient serving on different hardware platforms and improve the system efficiency with \\r\\noptimized  kernels  based  on  different  hardware  characteristics. Besides  inference  on  single  GPU  card, \\r\\nDeepSpeed  [12] also considers extending the  LLM  inference on  multiple  GPU cards  and  proposes \\r\\ntensor/pipeline/expert parallelism techniques. 3. Proposed Method \\r\\nIn this paper, we design an efficient LLM inference solution and implement it on Intel® GPU. To lower \\r\\nthe latency, we simplify the structure of Transformer decoder layer by reducing data movement operations \\r\\nand applying  customized kernel  fusion policy. To more  effectively  manage  the device memory  and \\r\\nimprove throughput, we propose a segment KV cache algorithm to make prompt key/value being shared \\r\\nbetween different response tokens. A customized SDPA kernel is designed to support our fusion policy \\r\\nand segment KV cache algorithm. 3.1 Model Structure Simplification \\r\\nAs  we  mentioned  above,  LLMs are  usually  complicatedly designed with  multiple  decoder  layers \\r\\nconsisting of massive operations to capture context information. The decoder layer usually has two basic \\r\\nmodules Multi   Head   Attention (MHA) and Feed-Forward   (FF),   which   are connected   by   some \\r\\nnormalization operations. Taking Llama2 for example, the basic model structure is shown in Figure 1. At \\r\\ntime  step 푡,  in  MHA module,  three Linear  operations  are  firstly adopted to  generate 퐐퐮퐞퐫퐲풕, 퐊퐞퐲풕,\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"56dbac1d-3be7-40cc-b8c4-14279a1ef278":{"__data__":"{\"id_\":\"56dbac1d-3be7-40cc-b8c4-14279a1ef278\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"2401.05391.pdf||PHMAujqg3eZ1PkPWK5PE7YXBgvpkcXYWt5aALI3o29s=\",\"metadata\":{},\"hash\":\"wg+dYSAIDLG2kHzuKcYiDG006wdKetfif4UKJW57HKo=\"}},\"hash\":\"VFu43mQ+dsZ2DSEWZ+GmtPn8/bZaIKkUy/P+zGNYpZs=\",\"text\":\"퐕퐚퐥퐮퐞풕, followed by possible RoPE for position embedding. Then SDPA module is applied for attention \\r\\ncontext computation, and finally another output Linear is utilized for feature projection. Before the SDPA \\r\\nmodule, some  data  movement  operations  including  Transpose, Cat and Index Select are  used. The \\r\\nTranspose operation is applied  on 퐐퐮퐞퐫퐲풕, 퐊퐞퐲풕, 퐕퐚퐥퐮퐞풕 for data  layout  transformation from \\r\\n[BS×BW,N푡,H,D] to [BS×BW,H,N푡,D],  where BS, BW indicate the  runtime batch  size, beam  width \\r\\nvalue, and H,D indicate the number of attention head and head dimension of the model, while N푡 indicates \\r\\nthe sequence length at time step 푡. Then past key/value (consisting of prompt 퐊퐞퐲ퟎ/퐕퐚퐥퐮퐞ퟎ  and response \\r\\n퐊퐞퐲ퟏ~풕−ퟏ / 퐕퐚퐥퐮퐞ퟏ~풕−ퟏ) are extracted by Index Select operations based on the 퐈퐧퐝퐢퐜퐞풕 tensor generated \\r\\nfrom Beam Search module, which will be concatenated with the current 퐊퐞퐲풕/퐕퐚퐥퐮퐞풕 to create KV cache \\r\\nfor attention context computation in SDPA module. The generated attention context in memory layout of \\r\\n[BS×BW,H,N푡,D] will be converted back to [BS×BW,N푡,H,D] for final Linear projection. LinearLinear\\r\\nQ-transV-trans\\r\\nSDP Module\\r\\nLinear\\r\\nQuery\\r\\nt\\r\\nValue\\r\\nt\\r\\nLinear\\r\\nK-trans\\r\\n K-Cat & V-Cat\\r\\nKey\\r\\nt\\r\\nAttn-trans\\r\\nRoPE ModuleRoPE Module\\r\\nAdd\\r\\nRMSNorm Module\\r\\nLinear\\r\\nSiLU\\r\\nLinear\\r\\nLinear\\r\\nAdd\\r\\nMul\\r\\nMHA\\r\\nFF\\r\\n         Decoder x L\\r\\nEmbedding\\r\\nRMSNorm Module\\r\\nIndex Select\\r\\nIndex Select\\r\\nLinear\\r\\nSoftmax\\r\\nPast Value\\r\\nPrompt \\r\\nV0\\r\\nResponse \\r\\nV1~Vt-1\\r\\nIndicet\\r\\nPrefill phase & Decoding phase\\r\\nPast Key\\r\\nPrompt \\r\\nK0\\r\\nResponse \\r\\nK1~Kt-1\\r\\n \\r\\nFigure 1. The standard flowchart of LLM inference with Llama2. The data movement operations in MHA like Transpose, Cat, Index Select labeled in orange in Figure 1 \\r\\nwill cause memory access and kernel launch overhead. In this paper, we simplify the MHA structure by \\r\\nreducing these operators as shown in Figure 2. At prefill phase, 퐐퐮퐞퐫퐲ퟎ, 퐊퐞퐲ퟎ, 퐕퐚퐥퐮퐞ퟎ in memory layout \\r\\nof [BS×BW,N0,H,D] (named  as  batch  first  layout) are generated, directly  followed  by a customized\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"8cf778f9-1718-4805-8ba8-d805ac2ca2f9":{"__data__":"{\"id_\":\"8cf778f9-1718-4805-8ba8-d805ac2ca2f9\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"2401.05391.pdf||0XBRAARuuqBKBmmcn4NR6XEfLtHhFWNuIN8pMZ1Um4I=\",\"metadata\":{},\"hash\":\"bZ4ZOdNjFNPMT3V2+rLlB2dZVj6eCGRTRxwZOz1rEcg=\"}},\"hash\":\"BYcvz15iZAKDpDFCzkb8Etdj9xHYXrpN5gwObwV6Drs=\",\"text\":\"designed SDPA kernel for attention  context computation without  any transformation operation. The \\r\\ncustomized designed SDPA kernel  accepts  input  and  generate  output tensors in batch  first  layout \\r\\n([BS×BW,N0,H,D]) to avoid the transformation overhead, as shown in Figure 2(a). At decoding phase, \\r\\nCat operations shown in Figure 1 are used to combine key/value generated at different time step together. To remove the Cat overhead, we pre-allocate KV cache buffers for response tokens with sequence length \\r\\nNstep in memory layout of [Nstep,BS×BW,H,D] (named as sequence first layout). At time step 푡 of the \\r\\ndecoding  phase, the 퐊퐞퐲풕 and 퐕퐚퐥퐮퐞풕 are  kept in corresponding parts  of the pre-allocated  buffers \\r\\n[N푡−1:N푡, BS×BW,H,D]. The  sequence  first  layout ensures the 퐊퐞퐲풕 and 퐕퐚퐥퐮퐞풕 are  always \\r\\ncontiguously arranged for easy computation unnecessary to aware of the hyper-parameter Nstep. Then the \\r\\nprompt 퐊퐞퐲ퟎ, 퐕퐚퐥퐮퐞ퟎ and  the  response key/value  (퐊퐞퐲ퟏ~풕, 퐕퐚퐥퐮퐞ퟏ~풕 in the  pre-allocated KV  cache \\r\\nbuffers) together  with  the 퐈퐧퐝퐢퐜퐞ퟏ~풕 tensor (generated by Beam Search  module) will  be  used  by our \\r\\ncustomized SDPA kernel to compute attention context in memory layout of [1,BS×BW,H,D] for final \\r\\nlinear computation without any transformation. To align the sequence first layout policy for KV cache \\r\\nwithout introducing extra data movement overhead, we convert the input tensor (named as Hidden States \\r\\nin Figure 2) before the first decoder layer from batch first layout to sequence first layout. The sequence \\r\\nfirst layout will be propagated internal and external all the decoder layers until arriving the last decoder \\r\\nlayer, the output of which will be converted back to batch first layout. The optimized MHA structure can \\r\\nbe  seen  in  Figure 2, illustrating that all  data  movement  operations labeled  in  orange  in  Figure  1 are \\r\\nremoved. OperationTensor\\r\\nLinearLinear\\r\\nLinear\\r\\nHidden States \\r\\nseq first layout\\r\\nLinear\\r\\nAttention Context\\r\\nseq first layout\\r\\nHidden States \\r\\nseq first layout\\r\\nFusedSDP\\r\\nRoPE Module\\r\\nQueryt\\r\\nRoPE Module\\r\\nKeytValuet\\r\\nMHA\\r\\nLinearLinear\\r\\nLinear\\r\\nHidden States \\r\\nbatch first layout\\r\\nLinear\\r\\nAttention Context\\r\\nbatch first layout\\r\\nHidden States \\r\\nbatch first layout\\r\\nFusedSDP\\r\\nRoPE Module\\r\\nQuery0\\r\\nRoPE Module\\r\\nKey0Value0\\r\\nMHA\\r\\nPrompt Key\\r\\nPrompt Value\\r\\n... V1...Vt-1Vt\\r\\nResponse Key\\r\\nResponse Value\\r\\nK1Kt-1Kt\\r\\nIndice1~t\\r\\nPrefill phaseDecoding phase\\r\\nPre-allocated KV cache\\r\\n \\r\\n(a)                                                                                (b) \\r\\nFigure 2. Optimized MHA module in Llama2 (a) at prefill phase, and (b) decoding phase, respectively. Moreover,  we respectively fuse  the multiple  operations  in RMSNorm,  RoPE and SDPA modules \\r\\n(labeled  in  pink in  Figure  1) to  single  kernels. The three  Linear  operations (respectively generating \\r\\nquery/key/value)  are also combined into a single one and the element-wise operations labeled in yellow \\r\\nin  Figure  1 are further fused with  their  previous  Linear  operations. The  optimized Llama2 structure  is \\r\\nshown in Figure 3. Compared with the original structure shown in Figure 1, the data movement operations \\r\\nlabeled in orange and the element-wise operations labeled in yellow are all removed. The modules labeled \\r\\nin   pink containing   multiple   operations   are   fused   into single   kernels. Even   we introduce extra\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"61741d4c-5a5d-4532-8a47-7840b5f875d7":{"__data__":"{\"id_\":\"61741d4c-5a5d-4532-8a47-7840b5f875d7\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"2401.05391.pdf||fhS37/t0afYTMesAWz1Rzdi5ZxlQH3pn1h2bx25uMso=\",\"metadata\":{},\"hash\":\"rQxtsqILNEQC3pyKsvCgv3F1HQS2dO7XqhgWe324vQM=\"}},\"hash\":\"XUPkkZONsmYoJV3VhwILH4YHkmZkB3k30civEU9mnfc=\",\"text\":\"sequence/batch first layout transformation operations at decoding phase, they happen only once before the \\r\\nfirst and last decoder layers at each time step, almost no overhead compared with the whole inference. With our optimization, the number of operations in each Llama2 decoder layer has been reduced to nine, \\r\\nmuch smaller than the original hundreds of operations. Decoder x L   \\r\\n     \\r\\nDecoder x L\\r\\nFusedSDP\\r\\nLinearAdd\\r\\nQKVLinear\\r\\nFusedRoPE\\r\\nFusedRMSNorm\\r\\nLinearSiLU\\r\\nLinearMul\\r\\nLinearAdd\\r\\nMHA\\r\\nFF\\r\\nFusedSDP\\r\\nLinearAdd\\r\\nQKVLinear\\r\\nFusedRoPE\\r\\nFusedRMSNorm\\r\\nLinearSiLU\\r\\nLinearMul\\r\\nLinearAdd\\r\\nMHA\\r\\nFF\\r\\nPrefill phaseDecoding phase\\r\\nFusedRMSNorm\\r\\nLinear\\r\\nSoftmax\\r\\nToSeqFirst\\r\\nToBatchFirst\\r\\nLinear\\r\\nPrompt Key\\r\\nPrompt Value\\r\\n... V1...Vt-1Vt\\r\\nResponse Key\\r\\nResponse Value\\r\\nEmbedding\\r\\nFusedRMSNorm\\r\\nK1Kt-1Kt\\r\\nEmbedding\\r\\nSoftmax\\r\\nIndicet\\r\\nQuery0\\r\\nQuery0 & Key0 Queryt & Keyt \\r\\nQueryt\\r\\n \\r\\n(a)                                                      (b) \\r\\nFigure 3. Optimized flowchart of LLM inference with Llama2 (a) at prefill phase, and (b) decoding phase, \\r\\nrespectively. 3.2 Segment KV Cache  \\r\\n  LLM inference is memory consuming caused by large parameter size and KV cache policy, limiting \\r\\nthe batch size value and then further impacting the system throughput. Assuming the LLM has L numbers \\r\\nof  decoder  layers, H attention  heads, D head  dimension, then  the  KV  cache  size  of  one  token  can  be \\r\\ncomputed as Eq (1), where 2 indicates the key and value. The runtime batch size, beam width are BS, BW, \\r\\nand the total sequence length is combined with prompt sequence length Nprompt and response sequence \\r\\nlength Nresponse. In  standard KV cache implementation,  the  prompt and  response key/value will  be \\r\\nconcatenated together to create contiguous KV cache in shape of [BS×BW,Nprompt+Nresponse,H,D]. Then the KV  cache size at  the  last  time step  can  be  computed  as  Eq  (2). In  standard  implementation, \\r\\nkeeping prompt and response key/value in contiguous KV cache buffers will waste device memory since \\r\\nthe prompt key/value should be extended BW times. Moreover, since the KV Cache buffers grow larger \\r\\nat each time step in decoding phase, new bigger buffers will be allocated while may not reuse the previous \\r\\nsmaller KV Cache buffers, resulting in many memory fragments. Considering the extreme situation, all \\r\\nthe  physical  device  memory  of  KV  cache at  each  time  step  cannot  be  reused,  then  the total memory\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"07fc4094-f845-422a-883c-1f218646e43e":{"__data__":"{\"id_\":\"07fc4094-f845-422a-883c-1f218646e43e\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"2401.05391.pdf||68yz/qH1CKcYhL0Gm7Lnh1vl7yLjRfi5tkA+I03QvYU=\",\"metadata\":{},\"hash\":\"lWPBxhZU2ezjIaFjoLmhPqz4TPV4cMuh6cVnyQXy6HQ=\"}},\"hash\":\"3djKWgRe7/626hjgITUKPQ84bIZhqhv5xsa/xtOcBdM=\",\"text\":\"fragment should be the sum of KV Cache at each time step, much larger than the KV cache itself at the \\r\\nlast time step. cachetoken= 2×L×H×D×sizeof(datatype)                                     (1) \\r\\ncachestandard= BS×BW×(Nprompt+Nresponse)×cachetoken                           (2) \\r\\n \\r\\nTo avoid keeping duplicated prompt key/value and try best to reduce the memory fragment, we propose \\r\\na segment KV cache policy, holding the prompt and response key/value in different buffers and regularly \\r\\nempty  cache  the  fragments at each  time  step. At prefill  phase,  the prompt key/value  in  shape  of \\r\\n[BS,Nprompt,H,D] for each decoder layer are generated and kept on the device as shown in Figure 3(a). Then, at decoding phase, we pre-allocate the response KV cache in shape of [Nstep,BS×BW,H,D] for \\r\\neach decoder layer. The response key/value at each time will be kept on the corresponding part of KV \\r\\ncache, as shown in Figure 3(b). With the proposed segment KV cache policy, the prompt key/value can \\r\\nbe share with different response tokens. In some  previous  works, Nstep is  usually  set  as  the  max  position  length  of  the  model  like  4096  in \\r\\nLlama2, which can help reduce the memory fragments while resulting in memory wasting if the runtime \\r\\nresponse has smaller sentence length. To fix this issue, we set Nstep value dynamically increasing with \\r\\nstep=16. Taking response length larger than 16 for example, at the 1st timestep (Nresponse=1), we pre-\\r\\nallocate the KV cache with Nstep=16. Then at the 17th timestep (Nresponse=17), Nstep will be set as \\r\\n32 and new KV cache will be allocated, the corresponding part of which will be fulfilled with the original \\r\\nKV cache data and the other part will keep the new key/value data. At the same time, the previous KV \\r\\ncache with Nstep=16 will be manually empty cached to improve the probability of memory reuse. In \\r\\nthis way, the length of the response KV cache will increase with step=16, reducing some data movement \\r\\noperations  of  each  time  step  while  not  introducing  much  memory  wasting or fragments. The  memory \\r\\nconsumption of the segment KV cache policy with Nstep increasing with step=16 can be computed as \\r\\nEq (3). cachesegment= BS×(Nprompt+ BW×Ceil(Nresponse\\r\\nstep\\r\\n) ×step)×cachetoken                          (3) \\r\\n \\r\\nTable 1. Different LLM Configurations \\r\\nModel Decoder Layer (L) Attention Heads (H) Head Dim (D) \\r\\nGPT-J-6B 32 32 128 \\r\\nLlama2-13B 40 40 128 \\r\\nOPT-30B 48 56 128 \\r\\nBloom-176B 70 112 128 \\r\\n \\r\\nTaking LLMs with different parameter sizes from 6B to 176B as shown in Table 1 for example, the \\r\\nruntime BW, Nprompt and Nresponse set  as 4,  1024,  1024,  respectively. Not  considering  the  memory \\r\\nfragment, the KV cache memory consumption in Float16 datatype with different BS values at the last time \\r\\nstep are shown in Figure 4, where standard and segment represent the standard KV Cache policy and our \\r\\nproposed  segment  KV cache  policy,  respectively. Taking  GPT-J-6B  for  example,  with BS=32,  the\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"ab43e7fc-73bf-4a9e-88ea-b5bdbcd67879":{"__data__":"{\"id_\":\"ab43e7fc-73bf-4a9e-88ea-b5bdbcd67879\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"2401.05391.pdf||tnNg4cV+xlEYfl/0P/oKvIptKfdU+5VeXX/aLGeYbt8=\",\"metadata\":{},\"hash\":\"gXSFARthyxMlnuji4dA6neUXgHj7xKvpw5YBTFWXh2I=\"}},\"hash\":\"DbdQPK2FbFQjAFEUEc3HFH9zrYAEDTObrTSlmBLRuLk=\",\"text\":\"proposed segment KV cache consumes 86GB device memory, only about 63% of the standard KV cache \\r\\npolicy with 137GB, saving 51GB device memory in total. Thus, for GPU hardware with constraint device \\r\\nmemory, the proposed segment KV cache policy consumes less memory, so that larger batch size value \\r\\ncan be adopted to improve system throughput. Figure 4. Memory consumption of standard KV cache and segment KV cache on different LLMs \\r\\n \\r\\n3.3 Customized SDPA Kernel \\r\\nInspired by FlashAttention [21] and FasterTransformer [27], we fuse all computation steps in SDPA \\r\\nmodule as well as the possible Index Select operations into a single kernel based on our segment KV cache \\r\\npolicy. For the customized SDPA kernel at decoding phase, the input tensors of query, prompt key/value \\r\\nand  response  key/value  are  in  shapes  of [1,BS×BW,H,D], [BS,Nprompt,H,D] and [Nresponse,BS×\\r\\nBW,H,D], respectively. The output tensor of attention context is in shape of [1,BS×BW,H,D]. In our \\r\\nimplementation, BS and H are paralleled in different GPU work groups (blocks). In a single GPU work \\r\\ngroup,  the input tensors  of query 퐐, prompt  key/value  (퐊퐩퐫퐨퐦퐩퐭, 퐕퐩퐫퐨퐦퐩퐭) and  response  key/value \\r\\n(퐊퐫퐞퐬퐩퐨퐧퐬퐞, 퐕퐫퐞퐬퐩퐨퐧퐬퐞) are in shapes of [1,BW,D], [Nprompt,D] and [Nresponse,BW,D], respectively. The \\r\\noutput tensor  of attention context O is in shape of [1,BW,D]. The customized  SDPA kernel  at  prefill \\r\\nphase is a special case of that at decoding phase, only using input tensors of query and prompt key/value \\r\\nfor attention context computation. We take the computation in a single work group of SDPA kernel at \\r\\ndecoding phase as an example to elaborate how we implement the kernel. Index select fusion. The beam indices tensor 퐈퐧퐝퐢퐜퐞ퟏ~풕  in shape of [BW,Nresponse] is obtained in the \\r\\nsame way as that in FasterTransformer [27]. Based on 퐈퐧퐝퐢퐜퐞ퟏ~풕, the actual response key/value on each \\r\\nbeam  position  at  different  time step are  selected. Taking  the 2nd beam  position  at the  1st time  step for \\r\\nexample shown in Figure 5(a), the indices value at this position is 3, then the response key value at this \\r\\nposition should be the 3rd item at 1st time step in key cache. Kernel implementation. In a single work group, 퐐 will be loaded into register at first. Then we for loop \\r\\nto load the corresponding parts of 퐊퐩퐫퐨퐦퐩퐭 and 퐕퐩퐫퐨퐦퐩퐭  at Nprompt dimension to compute the attention \\r\\ncontext like FlashAttention. In next step, we apply the index select process as mentioned above on KV \\r\\ncache  and  for  loop  to  load  the  corresponding  part  of 퐊퐫퐞퐬퐩퐨퐧퐬퐞 and 퐕퐫퐞퐬퐩퐨퐧퐬퐞 at both BW and Nresponse \\r\\ndimensions for attention context computation. Finally, the attention context value respectively computed \\r\\nbased on prompt and response key/value will be accumulated together to get the result and be written into \\r\\nthe HBM. The corresponding flow can be seen in Figure 5(b).\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"75d1f433-1cdc-4b7f-a696-1e8c07ece9f7":{"__data__":"{\"id_\":\"75d1f433-1cdc-4b7f-a696-1e8c07ece9f7\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"2401.05391.pdf||cuua4OU7/eRcqfCw+feXEHT7a3fxuFdex8NeWk/RVLI=\",\"metadata\":{},\"hash\":\"DCtkBGXbKSEiPGrUW/EiG1DzOD0PfGk1GdUYT1s9BuU=\"}},\"hash\":\"BCmoC7YlRztypgiCtlcqnzMI7gyInl4bh4YHVjL86aQ=\",\"text\":\"t1t2t3t4t5\\r\\nt1t2t3t4t5\\r\\nt1t2t3t4t5\\r\\nt1t2t3t4t5\\r\\nBeam x D\\r\\nN\\r\\nresponse\\r\\nkey Cache\\r\\nt1t2t3t4t5\\r\\nt1t2t3t4t5\\r\\nt1t2t3t4t5\\r\\nt1t2t3t4t5\\r\\nActual Response Key\\r\\n1\\r\\n3\\r\\n2\\r\\n4\\r\\n3\\r\\n4\\r\\n2\\r\\n2\\r\\n1\\r\\n1\\r\\n3\\r\\n4\\r\\n2\\r\\n4\\r\\n3\\r\\n1\\r\\n4\\r\\n2\\r\\n3\\r\\n4\\r\\nt1t2t3t4t5\\r\\nBeam indices\\r\\nBeam1\\r\\nBeam2\\r\\nBeam3\\r\\nBeam4\\r\\nK\\r\\nprompt\\r\\n \\r\\nt0t0t0t0t0t0t0\\r\\nQ1\\r\\nt0\\r\\nt0\\r\\nt0\\r\\nt0\\r\\nt0\\r\\nt0\\r\\nt0\\r\\nBeam x D\\r\\nQ2\\r\\nQ3\\r\\nQ4\\r\\nQ\\r\\nDxN\\r\\nBeam x N\\r\\nNxD\\r\\nBeam x D\\r\\ninner loop N\\r\\nprompt\\r\\n V\\r\\nprompt\\r\\nt1t2t3t4t5\\r\\nt1t2t3t4t5\\r\\nt1t2t3t4t5\\r\\nt1t2t3t4t5\\r\\ninner loop N\\r\\nresponse\\r\\nouter loop\\r\\n BW\\r\\n \\r\\nt1\\r\\nt2\\r\\nt3\\r\\nt4\\r\\nt5\\r\\nV\\r\\nresponse\\r\\nt1\\r\\nt2\\r\\nt3\\r\\nt4\\r\\nt5\\r\\nt1\\r\\nt2\\r\\nt3\\r\\nt4\\r\\nt5\\r\\nt1\\r\\nt2\\r\\nt3\\r\\nt4\\r\\nt5\\r\\ninner loop\\r\\n N\\r\\nresponse\\r\\nouter loop BW\\r\\nK\\r\\nresponse\\r\\n \\r\\nD\\r\\n1\\r\\nD\\r\\nD\\r\\nData in HBM\\r\\nO1\\r\\nO2\\r\\nO3\\r\\nO4\\r\\n+\\r\\nD\\r\\nD\\r\\nD\\r\\nD\\r\\nouter loop\\r\\n BW\\r\\n \\r\\n1\\r\\n1\\r\\n1\\r\\nD\\r\\nD\\r\\nD\\r\\ninner loop\\r\\n N\\r\\nprompt\\r\\nData in Register\\r\\n \\r\\n                        (a)                                                                                   (b) \\r\\nFigure 5. Implementation of customized SDPA kernel. (a) Index select; (b) Customized SDPA kernel based \\r\\nsegment KV cache policy. 4. Experiments \\r\\n    We implement our LLM inference solution on Intel® GPU and perform the experiments on a cluster of \\r\\n4× Intel® Data Center Max 1550 GPU cards with 2 Tiles per Card, 64 Xe-cores & 512 EUs per Tile. The \\r\\ndevice memory  per Tile  is 64GB with  effective  memory  bandwidth about 1000GB/s. These  GPUs are \\r\\nhosted on a 2x Intel® Xeon® 8480+ system running Ubuntu 22.04.3. Our optimization solution code is \\r\\nreleased in Intel®-Extension-for-Pytorch (v2.1.10) and our software stacks are publicly available here for \\r\\nperformance reproduction. In our experiments, if no specification, we will use the configuration of Float16 datatype, input prompt \\r\\nlength Nprompt=1024 and output response length Nresponse=128 for performance evaluation. At first, \\r\\nwe perform experiments on our customized SDPA kernel to show the kernel efficiency. Then we apply \\r\\nour inference solution on some popular LLMs listed in Table 1. For models with small parameter sizes \\r\\nGPT-J-6B and Llama2-13B, we run them on Max 1550 GPU single tile. For models with large parameter \\r\\nsizes OPT-30B and Bloom-176B, we respectively run them on Max 1550 GPU one card two tiles and four \\r\\ncards  eight  tiles. For the  large parameter  size models,  we  use  automatic Tensor Parallel  (autoTP)  [12] \\r\\nalgorithm to split model layers horizontally to run them across GPU devices. We  use  both  latency  and  throughput criteria for performance evaluation. The  latency includes two \\r\\nratios: first token latency indicating the latency of the first token generation in a batch, and next token \\r\\nlatency indicating  the  average latency of  the second  to  last  token generation in  a  batch. For inference \\r\\nthroughput evaluation, we figure out the largest batch size can be set and collect the corresponding latency. Then the throughput can be computed as Eq (4). Throughput=(BSmax×Nresponse)/latency                                              (4) \\r\\n \\r\\n4.1 SDPA Kernel Performance Evaluation\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"a7b0b44d-b90b-4c8e-8b3e-f73aa39010d7":{"__data__":"{\"id_\":\"a7b0b44d-b90b-4c8e-8b3e-f73aa39010d7\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"2401.05391.pdf||ahinDG5CS6JNHc0LykVHln7mmqxlEAJc4oBU80Y6RUs=\",\"metadata\":{},\"hash\":\"6U50nk7qOiZBbufUlwVfRIFLdiS03xQu58PrKbbHSIA=\"}},\"hash\":\"MDiWd97gQ1ntsec9K+CbhWWzTv1f+4vsNwj2mANwBUs=\",\"text\":\"We implement the customized SDPA kernel with the input tensors of prompt key/value and response \\r\\nkey/value in shapes of [BS,Nprompt,H,D] and [Nresponse,BS×BW,H,D], respectively. Taking Llama2-\\r\\n13B (H=40 and D=128) with BW=4 for  example,  the  effective  memory  bandwidth values of  the \\r\\ncustomized  SDPA kernel  with  different BS are shown  in  Figure  6. For BS=1,  our  customized  kernel \\r\\n(prompt with batch first layout and response with sequence first layout) achieves 727GB/s, only about \\r\\n72% of the peak memory bandwidth. There are 64 Xe-cores on Max 1550 GPU single tile, and at least 64 \\r\\nnumbers  of  work  groups  are  needed  to  fulfill  all  the  Xe-cores,  otherwise  only  low  hardware  resource \\r\\noccupancy and poor efficiency can be achieved. In our implementation, BS and H are parallelly computed \\r\\nin different work groups. With BS=1, the paralleled workgroup number is H=40, not fulfilling the total \\r\\n64 Xe-cores. Small workload size with BS=1 will waste hardware resource and results in poor efficiency. When BS becomes larger and equals 16, BS×H=640 workgroups are scheduled on 64 Xe-cores and we \\r\\nachieve high hardware resource occupancy with memory bandwidth 942GB/s. The kernel efficiency will \\r\\ndecrease  with BS continuing becoming  larger, because  for  response  key/value  we  for  loop Nresponse to \\r\\nload D elements with stride BS×BW, the bigger the stride value, the lower probability of cache hitting. We also perform experiment to compare SDPA kernel efficiency with prompt key/value in batch first \\r\\n(proposed method) and sequence first layout (aligned with response KV cache layout). As shown in Figure \\r\\n6,  for BS=1,  the  two  layouts  can  both  be  simplified  as [Nprompt,H,D] and  achieve  similar memory \\r\\nbandwidth   value about   720GB/s. For   larger BS,   the   prompt   key/value   in   batch   first   layout \\r\\n[BS,Nprompt,H,D] with  stride  H  has  higher  cache  hit  probability  than  that  in  sequence  first  layout \\r\\n[Nprompt,BS,H,D] with  stride BS×H,  so  that  our customized  SDPA  kernel  implemented with  prompt \\r\\nkey/value in batch first layout (different with KV cache layout) can achieve better performance efficiency. Figure 6. SDPA kernel performance with different batch sizes for different prompt key/value memory layout \\r\\n \\r\\n4.2 System Latency Evaluation \\r\\n4.2.1 Latency evaluation with different prompt lengths \\r\\nFirstly,  we  perform  experiments on  Llama2-13B to  figure  out  the influence  of  the prompt  sequence \\r\\nlength Nprompt on the latency with response tokens Nresponse=32,BS=1,BW=4. We collect the first \\r\\ntoken latency and the next token latency with different Nprompt from 32 to 4096. Performance results of \\r\\nthe standard HuggingFace (named as HF) and our proposed solution (Proposed) are shown in Figure 7, \\r\\namong which the HF implementation will out of memory for Nprompt=4096. From Figure 7(a) we can\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"a2d69a4b-410e-41e7-803f-3cae26b021c2":{"__data__":"{\"id_\":\"a2d69a4b-410e-41e7-803f-3cae26b021c2\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"2401.05391.pdf||kdLwvQdHeAvOXGhqdT/Jhep7xdgouLZHk1xSV6y76Lo=\",\"metadata\":{},\"hash\":\"CYBy+JOR7pHIbqVPsUnt7i9IvWQZHRRR5WFm8MNqvmE=\"}},\"hash\":\"XwTiAKrse+xmFI8aXKn7pDbmo/jMnXF9/r91DWY4N6w=\",\"text\":\"see  that, with Nprompt becomes  longer,  the  first  token  latency  of  HF  implementation becomes higher. Nprompt=128 is the inflection point, smaller than which the latency increases slowly with low hardware \\r\\noccupancy, while 2 times increasing for Nprompt larger than 128, almost achieving full hardware resource \\r\\noccupancy. Compared with HF implementation, the proposed method accesses only 1/BW amount data \\r\\nin HBM, and the corresponding inflection point happens at Nprompt=512, BW times longer than 128. Moreover, the proposed method achieves much lower first token latency than standard HF implementation \\r\\nfor  any Nprompt values,  and  more  than BW times when Nprompt larger  than 512  with  full  hardware \\r\\noccupancy. Figure 7(b) shows the same situations that the proposed method achieves much lower next \\r\\ntoken latency with any different prompt sequence length. (a)                                                                             (b) \\r\\nFigure 7. Llama2-13B latency of (a) the first token, and (b) the next token with different prompt sequence length, \\r\\nrespectively. 4.2.2 Latency evaluation on different LLMs \\r\\nWe  also  perform  experiment  to  evaluate  the  performance  influence  of  different  BW  values  (BW=\\r\\n1 and 4) with BS=1. The performance results of the first and next token latency on different LLMs are \\r\\nshown in Figure 8. Both the first and next token latency of the standard HF implementation grows linearly \\r\\nwith BW becomes larger. However, the BW value has little impact on the proposed method, with almost \\r\\nthe same first and next token latency with BW 4 and 1 as shown in Figure 8. Moreover, we compare the token latency of the proposed method and standard HF implementation. For \\r\\nBW 1, the first and next token latency of the proposed method achieves about 1.1x ~2x faster than the \\r\\nstandard  HF  implementation on  different  LLMs. The benefits  come  from  our  deep  fusion  policy with \\r\\nhighly efficient kernels, removing data movement operations and fusing element-wise operations, which \\r\\ncan help reduce both the HBM access and kernel launch overhead. For BW 4, the first token latency values \\r\\nof  our  proposed  method  are 4x ~7x  lower  than  the  standard  HF implementation. Besides  the  benefits \\r\\nfrom deep fusion policy, the segment KV cache method (unnecessary to extend the prompt with BW=4 \\r\\ntimes) helps reduce both memory access and kernel computation costs. The performance of next token \\r\\nlatency of the proposed method is also much lower than the HF implementation especially for BW=4 as \\r\\nshown in Figure 8(b).\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"cc0cc649-f4b0-4459-ba43-282aa91db279":{"__data__":"{\"id_\":\"cc0cc649-f4b0-4459-ba43-282aa91db279\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"2401.05391.pdf||ZgqiqrnqxytWGHAvVDkRtCJAufj7OM6Dq65qoWqaiM0=\",\"metadata\":{},\"hash\":\"v5PIz6oDJZPigg3WR25t0Xd7vFh5EZINsfDVk4t8g4s=\"}},\"hash\":\"pZ/3vK7i8toP+dRhrh1zbjyptqh9SsSwU76aosqBusM=\",\"text\":\"(a)                                                                        (b) \\r\\nFigure 8. (a) First and (b) next token latency performance of different LLMs with different parameters sizes \\r\\n \\r\\n4.3 System Throughput Evaluation \\r\\n \\r\\nFigure 9. Throughput performance of different LLMs with different parameters sizes \\r\\n \\r\\nWe  further  compare  our  proposed  LLM  inference  solution  with  standard  HF  implementation  for \\r\\nthroughput criteria computed by Eq (4). The performance results are shown in Figure 9, illustrating that \\r\\nthe proposed method achieves 8x ~27x improvement on throughput. Besides the optimization for latency, \\r\\nthe main benefit comes from our segment KV cache which can help save device memory then enlarge the \\r\\nruntime batch size value to improve the hardware resource utilization. 5. Conclusion \\r\\n     In this paper, we propose an efficient LLM inference solution with low latency and high throughput. Since LLM inference is a memory bound task, reducing the memory access frequency can help improve \\r\\nefficiency. The LLMs usually consist of multiple decoder layer, and we focus on simplifying the decoder\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"291847c9-5083-4aed-9446-baafe0c7935a":{"__data__":"{\"id_\":\"291847c9-5083-4aed-9446-baafe0c7935a\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"2401.05391.pdf||N96CZB5UOzfpjjxBj/pX8sh6diAYTORnU5FhZ98VOug=\",\"metadata\":{},\"hash\":\"/xPe2sfcb+B+BovAL4SCME6k6imUcbEFFVauLWheWnU=\"}},\"hash\":\"nsilPdtYydWzdZfqj3fgdi+g9C55RIVxjAiTi33l3go=\",\"text\":\"layer  structure  by  fusing data  movement  and  element-wise  operations to  reduce  memory  access. Considering that LLM inference works at auto-regressive mode and consumes large device memory, we \\r\\npropose a segment KV cache policy to make the prompt being shared between different responses and \\r\\nregularly empty cache the fragments, so that to save device memory and improve the throughput. We also \\r\\ndesign  an  efficient  SDPA kernel based  on our  fusion  policy  and the  segment  KV  cache method. The \\r\\nproposed  solution has  been verified  on several  popular  LLMs  with  parameter  sizes  from  6B~176B on \\r\\nIntel® GPU. The experimental results show that we achieve up to 7x lower latency and up to 27x higher \\r\\nthroughput than the standard HuggingFace implementation. Acknowledgements \\r\\nWe much appreciate the contributions of Xiaodong Qiu, Cong li, Zhong Cao and other Intel® engineers \\r\\nto our work. We also want to give thanks to the technical guidance from Eric Lin, Fangwen Fu, Patric \\r\\nZhao, Jiong Gong and other Intel® Architects. Thanks to the comments from Fan Zhao, Sophie Chen, \\r\\nTing Ye, Kristina Kermanshahche and Brian Golembiewski to our paper. Notices and Disclaimers \\r\\n• Performance varies by use, configuration and other factors. Learn more on the Performance Index \\r\\nsite. • Performance results are based on testing as of dates shown in configurations and may not reflect all \\r\\npublicly available updates. No product or component can be absolutely secure. • Your costs and results may vary. • Intel technologies may require enabled hardware, software or service activation. • © Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation \\r\\nor its subsidiaries. Other names and brands may be claimed as the property of others.\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"1db7a1aa-9845-44c2-bdf7-7725be1ef807":{"__data__":"{\"id_\":\"1db7a1aa-9845-44c2-bdf7-7725be1ef807\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"2401.05391.pdf||PN27Am8tNMOXdDexl0RPsb7mp3lm16abkKdNgQRC11g=\",\"metadata\":{},\"hash\":\"roIvSBqUE4Vwp/gh04EjAOuMN1BOklwoA1MGAN7tBuo=\"},\"NEXT\":{\"nodeId\":\"4ff7662b-9440-4d99-95fb-f3fcb7839d30\",\"metadata\":{},\"hash\":\"YmVKLjKcpeVVKepAkvXlf3GJgK6h29/TGH9PyjvLuaU=\"}},\"hash\":\"faGMJ8lgqkal6fczmqkec2o59NsMtbXi8yZVSeo3h3I=\",\"text\":\"Reference \\r\\n \\r\\n  [1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, \\r\\n“Attention is all you need,” in Advances in neural information processing systems, 30:5998–6008, \\r\\n2017. [2] OpenAI. “Gpt-4 technical report,” arXiv:2303.08774, 2023. [3] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy, K. McDonell, \\r\\nJ. Phang, M. Pieler, U. S. Prashanth, S. Purohit, L. Reynolds, J. Tow, B. Wang, and S. Weinbach, \\r\\n“GPT-NeoX20B: An open-source autoregressive language model,”  arXiv:2204.06745, 2022. [4] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. “Llama: Open and efficient \\r\\nfoundation language models,” arXiv:2302.13971, 2023. [5] OpenAI. 2022. https://openai.com/blog/chatgpt \\r\\n[6] Github. 2022. https://github.com/features/copilot \\r\\n[7] Google. 2023. https://bard.google.com/ \\r\\n[8] OpenAI. 2020. https://openai.com/blog/openai-api \\r\\n[9] Amazon Web Services. 2023. https://aws.amazon.com/bedrock/  \\r\\n[10] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. “Language models are few-shot learners,” Advances in neural information \\r\\nprocessing systems,” 33:1877–1901, 2020. [11] H. Wang, S. Ma, L. Dong, S. Huang, D. Zhang, and F. Wei. “Deepnet: Scaling transformers to 1,000 \\r\\nlayers,” arXiv:2203.00555, 2022. [12] R. Y. Aminabadi, S. Rajbhandari, M. Zhang, A. A. Awan, C. Li, D. Li, E. Zheng, J. Rasley, S. Smith, \\r\\nO. Ruwase, Y. He. “DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at \\r\\nUnprecedented Scale,” arXiv:2207.00032, 2022. [13]  R. Pope,  S. Douglas,  A. Chowdhery,  J. Devlin,  J. Bradbury,  A. Levskaya,  J. Heek,  K. Xiao,  S. Agrawal, and J. Dean. “Efficiently Scaling Transformer Inference,” arXiv:2211.05102, 2022. [14]  H. Touvron,  L. Martin,  K. Stone,  P. Albert,  A. Almahairi,  Y. Babaei,  N. Bashlykov,  S. Batra,  P. Bhargava, S. Bhosale, “Llama 2: Open foundation and fine-tuned chat models,” arXiv:2307.09288, \\r\\n2023. [15] I. Sutskever, O. Vinyals, and Q.\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"4ff7662b-9440-4d99-95fb-f3fcb7839d30":{"__data__":"{\"id_\":\"4ff7662b-9440-4d99-95fb-f3fcb7839d30\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"2401.05391.pdf||PN27Am8tNMOXdDexl0RPsb7mp3lm16abkKdNgQRC11g=\",\"metadata\":{},\"hash\":\"roIvSBqUE4Vwp/gh04EjAOuMN1BOklwoA1MGAN7tBuo=\"},\"PREVIOUS\":{\"nodeId\":\"1db7a1aa-9845-44c2-bdf7-7725be1ef807\",\"metadata\":{},\"hash\":\"faGMJ8lgqkal6fczmqkec2o59NsMtbXi8yZVSeo3h3I=\"}},\"hash\":\"YmVKLjKcpeVVKepAkvXlf3GJgK6h29/TGH9PyjvLuaU=\",\"text\":\"Sutskever, O. Vinyals, and Q. V Le. “Sequence to sequence learning with neural networks,” \\r\\narXiv:1409.3215, 2014. [16] Wang,  Ben,  and  Aran  Komatsuzaki. \\\"GPT-J-6B:  A  6  billion  parameter  autoregressive  language \\r\\nmodel. \\\" (2021). [17] B. Zhao, Q. Cui, R. Song, Y. Qiu, J. Liang. “Decoupled knowledge distillation,” In IEEE/CVF \\r\\nConference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June \\r\\n18-24, 2022, pages 11943–11952. IEEE, 2022. [18] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin. “Opt: Open pre-trained transformer language models,” arXiv:2205.01068, 2022. [19] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow,  ́ R. Castagné, A. S. Luccioni, F. Yvon, \\r\\nM. Gallé. “Bloom: A 176bparameter open-access multilingual language model,” arXiv:2211.05100, \\r\\n2022. [20] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz,  J. Davison,  S. Shleifer,  P. V. Platen,  C. Ma,  Y. Jernite,  J. Plu,  C. Xu,  T. L. Scao,  S. Gugger,  M. Drame,  Q. Lhoest,  A. M. Rush. “Transformers:  State-of-the-Art  Natural  Language\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"2c367b1d-3edd-4de6-bdab-c56d0e114216":{"__data__":"{\"id_\":\"2c367b1d-3edd-4de6-bdab-c56d0e114216\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"2401.05391.pdf||AFYEcw7idVLwi4JQtpVAhUEzH8E+vAqkNqT0u+IjbQ0=\",\"metadata\":{},\"hash\":\"WENLqrRRO4Tr8x5Oan/0ag6yYcj+++9qw4/7vvtJtyg=\"},\"NEXT\":{\"nodeId\":\"6ff95514-ceb9-4d8e-8b45-4bf7abc14b83\",\"metadata\":{},\"hash\":\"oVFJRgfUu0b97tw24FeDKWWzxPAdHabjXwP6HMN5is0=\"}},\"hash\":\"BvFtGE1XcFnGnROh2aZ19i+ZoWOiBDkEcF9kyNBuZ0c=\",\"text\":\"Processing,”  Proceedings  of  the  2020  Conference  on  Empirical  Methods  in  Natural  Language \\r\\nProcessing: System Demonstrations oct. 2020. [21] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, C. Ré. “FlashAttention: Fast and Memory-Efficient Exact \\r\\nAttention with IO-Awareness,” arXiv:2205.14135, 2022. [22] A. Ivanov, N. Dryden, T. Ben-Nun, S. Li, and T. Hoefler. “Data movement is all you need: A case \\r\\nstudy on optimizing transformers,” Proceedings of Machine Learning and Systems, 3: 711–732, 2021. [23] M. Li, Y. Liu, X. Liu, Q. Sun, X. You, H. Yang, Z. Luan, L. Gan, G. Yang, and D. Qian. “The deep \\r\\nlearning compiler: A comprehensive survey,” IEEE Transactions on Parallel and Distributed Systems, \\r\\n32(3):708–727, 2020. [24]   ONNX   Runtime   developers,   “ONNX   Runtime,”   11   2018. [Online]. Available: \\r\\nhttps://github.com/microsoft/onnxruntime \\r\\n[25] TensorFlow XLA developers, “Xla: Optimizing compiler for machine learning. ” [Online]. Available: \\r\\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/compiler/xla \\r\\n[26] J. Fang, Y. Yu, C. Zhao, J. Zhou. “TurboTransformers: An Efficient GPU Serving System For \\r\\nTransformer Models,” arXiv:2010.05680, 2020. [27] NVIDIA. 2023. FasterTransformer. https://github.com/NVIDIA/FasterTransformer \\r\\n[28] M. Santacroce, Z. Wen, Y. Shen, Y. Li. “What matters in the structured pruning of generative \\r\\nlanguage models? ” arXiv:2302.03773, 2023. [29] T. Dettmers, M. Lewis, Y. Belkada, L. Zettlemoyer. “Llm.int8(): 8-bit  matrix  multiplication  for \\r\\ntransformers at scale,” arXiv:2208.07339, 2022. [30] G. Park, B. Park, M. Kim, S. Lee, J. Kim, B. Kwon, S. J. Kwon, B. Kim, Y. Lee, D. Lee. “LUT-\\r\\nGEMM:  Quantized  Matrix  Multiplication  based  on  LUTs  for  Efficient  Inference  in  Large-Scale \\r\\nGenerative Language Models,” arXiv:2206.09557, 2022. [31] Y. Idelbayev and M. Á. Carreira-Perpiñán, \\\"Low-Rank Compression of Neural Nets: Learning the \\r\\nRank  of  Each  Layer,\\\"  2020  IEEE/CVF  Conference  on  Computer  Vision  and  Pattern  Recognition \\r\\n(CVPR), Seattle, WA, USA, 2020, pp. 8046-8056, doi: 10.1109/CVPR42600.2020.00807. [32] R. Child,  S. Gray,  A. Radford,  and  I. Sutskever,  “Generating  long  sequences  with  sparse \\r\\ntransformers,” arXiv:1904.10509, 2019. [33] W. Kwon,  Z. Li,  S. Zhuang,  Y. Sheng,  L. Zheng,  C. H. Yu,  J. E. Gonzalez,  H. Zhang,  I. Stoica. “Efficient  Memory  Management  for  Large  Language  Model  Serving  with  PagedAttention,” \\r\\narXiv:2309.06180, 2023. [34] Yu, G. I., Jeong JSeong, G. W. Kim, S. Kim, and B. G. Chun.\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"6ff95514-ceb9-4d8e-8b45-4bf7abc14b83":{"__data__":"{\"id_\":\"6ff95514-ceb9-4d8e-8b45-4bf7abc14b83\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"2401.05391.pdf||AFYEcw7idVLwi4JQtpVAhUEzH8E+vAqkNqT0u+IjbQ0=\",\"metadata\":{},\"hash\":\"WENLqrRRO4Tr8x5Oan/0ag6yYcj+++9qw4/7vvtJtyg=\"},\"PREVIOUS\":{\"nodeId\":\"2c367b1d-3edd-4de6-bdab-c56d0e114216\",\"metadata\":{},\"hash\":\"BvFtGE1XcFnGnROh2aZ19i+ZoWOiBDkEcF9kyNBuZ0c=\"}},\"hash\":\"oVFJRgfUu0b97tw24FeDKWWzxPAdHabjXwP6HMN5is0=\",\"text\":\"W. Kim, S. Kim, and B. G. Chun. \\\"Orca: A Distributed Serving System \\r\\nfor  Transformer-Based  Generative  Models. \\\"  In  16th  USENIX  Symposium  on  Operating  Systems \\r\\nDesign and Implementation (OSDI 22). USENIX Association, 2022. [35] T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Yan, H. Shen, M. Cowan, L. Wang, Y. Hu, L. Ceze et al., \\r\\n“TVM:  An  automated  End-to-End  optimizing  compiler  for  deep  learning,”  in  13th  USENIX \\r\\nSymposium on Operating Systems Design and Implementation (OSDI 18), 578–594, 2018.\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"99f6f9a7-46f0-4c3a-a80d-7480f937b103":{"__data__":"{\"id_\":\"99f6f9a7-46f0-4c3a-a80d-7480f937b103\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"corporate_homedepot_com_.txt||14kna\",\"metadata\":{},\"hash\":\"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY=\"},\"NEXT\":{\"nodeId\":\"78730d49-754b-4cc7-96a4-fa3192cb694c\",\"metadata\":{},\"hash\":\"PelEE+YkRcP6ATTylgcaZ/7iEjuEJzws22L9dapQ/IQ=\"}},\"hash\":\"E+CrBYCmHFrmkQJOrs91WMMpddQzr9cyh+BfhmVaMgU=\",\"text\":\"Skip to main content\\n    \\n    \\n      \\n    \\n\\n  \\n    \\n            \\n        \\n          \\n            \\n          \\n          \\n            \\n          \\n          \\n            \\n               Shop\\n               Stores\\n               Contact\\n                              \\n  \\n      Search\\n    \\n      \\n      \\n\\n  \\n  \\n\\n\\n\\n\\n\\n  \\n          Keywords\\n                    \\n\\n                      \\n\\n\\n\\n\\n\\n\\n  \\n          Type\\n                    \\n- Any -NewsPagesDocuments\\n                      \\n\\n\\n\\n\\n\\n\\n  \\n          Sort\\n                    \\nRelevanceNewest firstOldest firstA to ZZ to ARelevance Asc\\n                      \\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n\\n               Customer Service 1 (800) 466-3337\\n            \\n          \\n          \\n            \\n              The Home Depot\\n            \\n          \\n        \\n      \\n                            \\n                    \\n                          \\n        The Home Depot\\n  \\n\\n            \\n  Main navigation\\n  \\n\\n        \\n              \\n                                      \\n                          Menu\\n              \\n                                  \\n                          About Us\\n                        \\n                                      \\n                          About Us\\n              \\n                                  \\n                          Our Story\\n              \\n                                  \\n                          Leadership\\n              \\n                                  \\n                          Timeline & History\\n              \\n                                  \\n                          Sponsorship & Paid Media\\n              \\n                                  \\n                          Economic Impact\\n              \\n                                  \\n                          Values Wheel\\n              \\n        \\n  \\n              \\n                                  \\n                          Newsroom\\n                        \\n                                      \\n                          Newsroom\\n              \\n                                  \\n                          Gallery\\n              \\n                                  \\n                          News Releases\\n              \\n        \\n  \\n              \\n                                  \\n                          Investor Relations\\n                        \\n                                      \\n                          Investor Relations\\n              \\n                                  \\n                          Stock Information\\n                        \\n                                      \\n                          Stock Quote & Chart\\n              \\n                                  \\n                          Historical Lookup\\n              \\n                                  \\n                          Investment Calculator\\n              \\n                                  \\n                          Stock Splits\\n              \\n                                  \\n                          Dividend History\\n              \\n                                  \\n                          Analyst Coverage\\n              \\n        \\n  \\n              \\n                                  \\n                          Financial Reports\\n                        \\n                                      \\n                          Current Forms\\n              \\n                                  \\n                          Quarterly Earnings\\n              \\n                                  \\n                          Annual Reports\\n              \\n                                  \\n                          SEC Filings\\n              \\n        \\n  \\n              \\n                                  \\n                          News Releases\\n              \\n                                  \\n                          Events & Presentations\\n              \\n                                  \\n                          Corporate Governance\\n                        \\n                                      \\n                          Overview\\n              \\n                                  \\n                          Factsheet\\n              \\n                                  \\n                          Board of Directors\\n              \\n                                  \\n                          Committee Members & Charters\\n              \\n        \\n  \\n              \\n                                  \\n                          Shareholder Services\\n                        \\n                                      \\n                          Direct Stock Purchase Plan\\n              \\n                                  \\n                          Stock Transfer Agent\\n              \\n                                  \\n                          Account Access\\n              \\n                                  \\n                          Employee Stock Plan Contact\\n              \\n                                  \\n                          E-Proxy Materials\\n              \\n        \\n  \\n              \\n                                  \\n                          Investor Resources\\n                        \\n                                      \\n                          ESG Investors\\n              \\n                                  \\n                          2019 Investor and Analyst Conference\\n              \\n                                  \\n                          Contact Investor Relations\\n              \\n                                  \\n                          Investor Packet\\n              \\n                                  \\n                          FAQs\\n              \\n                                  \\n                          Email Alerts\\n              \\n                                  \\n                          Requested Printed Materials\\n              \\n        \\n  \\n              \\n                                  \\n                          ESG Investors\\n              \\n        \\n  \\n              \\n                                  \\n                          Foundation\\n                        \\n                                      \\n                          Foundation\\n              \\n                                  \\n                          Serving Veterans\\n              \\n                                  \\n                          Disaster Relief\\n              \\n                                  \\n                          Path to Pro\\n              \\n                                  \\n                          Grants\\n              \\n        \\n  \\n              \\n                                  \\n                          Responsibility\\n                        \\n                                      \\n                          Responsibility\\n              \\n                                  \\n                          2023 ESG Report\\n              \\n                                  \\n                          Resources & Reports\\n              \\n                                  \\n                          Diversity, Equity and Inclusion\\n              \\n                                  \\n                          Supplier Diversity\\n              \\n                                  \\n                          Political Engagement\\n              \\n        \\n  \\n              \\n                                  \\n                          Careers\\n              \\n                                  \\n                          Search\\n              \\n                                  \\n                          Shop Online\\n              \\n                                  \\n                          Contact Us\\n              \\n                                  \\n                          Store Finder\\n              \\n        \\n  \\n\\n\\n  \\n\\n\\n\\n                                            \\n                  \\n          \\n            \\n              \\n          \\n          \\n              \\n                \\n                  \\n                                      \\n                        \\n  \\n    \\n      \\n      \\n                                  \\n        \\n        \\n  \\n                Infographic: The Home Depot Announces Fourth Quarter 2023 Results\\n\\n                                            \\n        \\n        \\n  \\n                The Home Depot Announces Fourth Quarter and Fiscal 2023 Results; Increases Quarterly Dividend by 7.7%; Provides Fiscal 2024 Guidance\\n\\n                                            \\n        \\n        \\n  \\n                Beyond Words: Regional VP Talks Embracing Diversity and American Sign Language in the Workplace\\n\\n                                            \\n        \\n        \\n  \\n                The Home Depot Named on Fortune’s 2024 Most Admired Companies List, Ranking #1 in Specialty Retail for the Second Year\\n\\n                \\n          \\n        Previous\\n                Next\\n      \\n    \\n        \\n        \\n        \\n        \\n\\n          \\n        Previous\\n                Next\\n      \\n    \\n\\n\\n    \\n  \\n          \\n\\n\\n                      \\n                      \\n                        \\n  \\n    \\n      \\n      \\n          \\n    \\n     \\n        \\n  \\nInfographic: The Home Depot Announces Fourth Quarter 2023 Results  \\n  \\n    \\n    \\n     \\n        \\n  \\nThe Home Depot Announces Fourth Quarter and Fiscal 2023 Results; Increases Quarterly Dividend by 7.7%; Provides Fiscal 2024 Guidance  \\n  \\n    \\n    \\n     \\n        \\n  \\nBeyond Words: Regional VP Talks Embracing Diversity and American Sign Language in the Workplace  \\n  \\n    \\n    \\n     \\n        \\n  \\nThe Home Depot Named on Fortune’s 2024 Most Admired Companies List, Ranking #1 in Specialty Retail for the Second Year  \\n  \\n\\n    \\n  \\n          \\n\\n\\n                      \\n                                                        \\n  \\n    \\n      \\n      \\n  \\n    \\n      \\n      \\n   \\n        NEWSROOM HIGHLIGHTS: \\n         LATEST STORIES & ARTICLES\\n   \\n   \\n       VISIT OUR NEWSROOM\\n   \\n\\n    \\n      \\n      \\n      \\n            \\n                  \\n            \\n \\n   \\n     \\n        Earnings\\n   \\n  February 20, 2024\\n  Infographic: The Home Depot Announces Fourth Quarter 2023 Results \\n  \\n \\n\\n\\n          \\n                  \\n            \\n \\n   \\n     \\n        Earnings\\n   \\n  February 20, 2024\\n  The Home Depot Announces Fourth Quarter and Fiscal 2023 Results; Increases Quarterly Dividend by 7.7%; Provides Fiscal 2024 Guidance \\n  \\n \\n\\n\\n          \\n                  \\n            \\n \\n   \\n     \\n        Diversity Equity Inclusion\\n   \\n  February 14, 2024\\n  Beyond Words: Regional VP Talks Embracing Diversity and American Sign Language in the Workplace \\n  \\n \\n\\n\\n          \\n                  \\n            \\n \\n   \\n     \\n        Company\\n   \\n  February 08, 2024\\n  The Home Depot Named on Fortune’s 2024 Most Admired Companies List, Ranking #1 in Specialty Retail for the Second Year \\n  \\n \\n\\n\\n          \\n              \\n      \\n\\n    \\n  \\n          \\n\\n\\n    \\n  \\n\\n  \\n    \\n      \\n      \\n            Our Story\\n\\n \\n\\n      \\n    \\n  \\n\\n  \\n    \\n      \\n      \\n             \\n\\n\\n\\n\\n  \\n    \\n        \\n      \\n  \\n    \\n              \\n\\n\\n\\n      \\n            Responsibility\\n      \\n  \\n\\n\\n\\n\\n\\n\\n\\n  \\n    \\n        \\n      \\n  \\n    \\n              \\n\\n\\n\\n      \\n            History\\n      \\n  \\n\\n\\n\\n\\n\\n\\n\\n  \\n    \\n        \\n      \\n  \\n    \\n              \\n\\n\\n\\n      \\n            Team Depot\\n      \\n  \\n\\n\\n\\n\\n \\n\\n      \\n    \\n  \\n\\n  \\n    \\n      \\n      \\n  \\n    \\n      \\n      \\n          \\n      \\n\\n  \\n\\n \\n\\n  \\n\\n\\n  \\n\\n    \\n  \\n          \\n\\n\\n    \\n  \\n\\n  \\n    \\n      \\n      \\n\\n\\n  \\n    \\n        \\n      \\n  \\n    \\n\\n\\n            \\n      \\n            https://live-home-depot-corporate.pantheonsite.io/\\n      \\n  \\n\\n\\n    \\n  \\n\\n  \\n    \\n      \\n      \\n  \\n    \\n      \\n      \\n          \\n    \\n  \\n\\n    \\n  \\n          \\n\\n\\n    \\n  \\n\\n  \\n    \\n      \\n      \\n  \\n    \\n      \\n      \\n          \\n\\n\\n\\n\\n    \\n  \\n          \\n\\n\\n    \\n  \\n\\n  \\n    \\n      \\n      \\n  \\n    \\n      \\n      \\n        \\n            \\n                  \\n            \\n          \\n              \\n      \\n\\n    \\n  \\n          \\n\\n\\n    \\n  \\n\\n\\n                \\n              \\n                                  \\n        \\n          \\n        \\n              \\n          \\n\\n                        \\n                                                  \\n                      \\n    \\n      \\n  Corporate Info\\n  \\n\\n        \\n              \\n                    \\n                About Us\\n              \\n                \\n                Investor Relations\\n              \\n                \\n                THD Canada\\n              \\n                \\n                THD Mexico\\n              \\n                \\n                Careers\\n              \\n                \\n                Suppliers\\n              \\n                \\n                Privacy & Security Statement\\n              \\n        \\n  \\n\\n\\n  \\n\\n  \\n\\n                  \\n                              \\n              \\n                            \\n                \\n                  \\n                    \\n                    X\\n                    \\n                  \\n                  \\n                    \\n                    Facebook\\n                    \\n                  \\n                  \\n                    \\n                    LinkedIn\\n                    \\n                  \\n                  \\n                    \\n                    YouTube\\n                    \\n                  \\n                  \\n                    \\n                    Instagram\\n                    \\n                  \\n                  \\n                \\n              \\n            \\n            \\n            \\n              \\n                © 2024\\n                Home Depot Product Authority,\\n                LLC All Rights Reserved.\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"78730d49-754b-4cc7-96a4-fa3192cb694c":{"__data__":"{\"id_\":\"78730d49-754b-4cc7-96a4-fa3192cb694c\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"corporate_homedepot_com_.txt||14kna\",\"metadata\":{},\"hash\":\"d8xHMWX2069LNSOebgHKXLKRqwayD6AQrX/GyhfVtnY=\"},\"PREVIOUS\":{\"nodeId\":\"99f6f9a7-46f0-4c3a-a80d-7480f937b103\",\"metadata\":{},\"hash\":\"E+CrBYCmHFrmkQJOrs91WMMpddQzr9cyh+BfhmVaMgU=\"}},\"hash\":\"PelEE+YkRcP6ATTylgcaZ/7iEjuEJzws22L9dapQ/IQ=\",\"text\":\"Use of this site is subject to certain Terms of Use.\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"81155eae-fc19-4ab0-bd03-531194a5ecde":{"__data__":"{\"id_\":\"81155eae-fc19-4ab0-bd03-531194a5ecde\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"essay.pdf||4YcMzZLR/BL09BruoFDGPRA2vb8xcRXtFvZtKsIVjDA=\",\"metadata\":{},\"hash\":\"1UoYTTYohqdtk4DkVFCjTZm0soevXTvAE5WnseNPYPw=\"}},\"hash\":\"xlS3BgjAApHX+sIhrghm85pWtrUauCWXQf/g+HCGW3k=\",\"text\":\"1\\r\\n\\u0001Identity and the \\r\\nComing-of-Age Narrative\\r\\nA great deal has been said about the heart of a girl when she\\r\\nstands “where the brook and river meet,” but what she feels\\r\\nis negative; more interesting is the heart of a boy when just\\r\\nat the budding dawn of manhood. —James Weldon Johnson, The Autobiography\\r\\nof an Ex-Coloured Man\\r\\nLittle by little it has become clear to me that every great\\r\\nphilosophy has been the confession of its maker, as it were\\r\\nhis involuntary and unconscious autobiography. —Friedrich Nietzsche, Beyond Good and Evil\\r\\n“Read this—you’ll love it. ” With these words, my mother hands me the\\r\\ncopy of Little Womenher aunt gave her when she was twelve and\\r\\nplants a seed that, three decades later, will grow into this study. At age twelve,\\r\\nI am beginning to balk at everythingmy mother suggests, but being constitu-\\r\\ntionally unable to resist any book, I grudgingly begin to read Louisa May\\r\\nAlcott’s novel. And like generations of girls before me, I fall in love with\\r\\nfunny, feisty Jo and weep when Beth dies. Like Jo, I try to be good, but I\\r\\nhave big dreams that distract me from my daily duties. Jo voices my own\\r\\nsecret desires when she tells her sisters that she wants to do “something\\r\\nheroic or wonderful that won’t be forgotten when I’m dead. I don’t know\\r\\nwhat, but I’m on the watch for it and mean to astonish you all someday”\\r\\n(172). My mother keeps me steadily supplied with “underground” litera-\\r\\nture—girls’ books—and while I feign disinterest, she is right. I do love these\\r\\nbooks, all of them: Alcott’s sequels to Little Women; Anne Frank’s diary; Jane\\r\\n1\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"6c5773ed-a2cd-4293-ae91-92813a73a125":{"__data__":"{\"id_\":\"6c5773ed-a2cd-4293-ae91-92813a73a125\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"essay.pdf||mg591T7N8x8xBVVfOu26Q6GDKhdlU032K6IvJVBAZvg=\",\"metadata\":{},\"hash\":\"X6rscDaCU/pNL/GGb0gNPFRJHemM1g+NKvow1YLHJYo=\"}},\"hash\":\"m+J3CjFsmQ8As3Fb1VVa/6KtHucw1fJ8Plct5490Jqw=\",\"text\":\"Eyre; Willa Cather’s complete works. At fourteen, I ask my ninth-grade Eng-\\r\\nlish teacher if we will be reading Cather, but her face tells me she has never\\r\\nheard of my current favorite. I will have to wait twenty years, until graduate\\r\\nschool, before I can talk about Cather in school. While coming of age, my reading life is thus split between school read-\\r\\ning and private reading, but it is women’s literature that feeds my soul. It also\\r\\nconfuses me. Many of the books my mother gives me glorify goodness, duty,\\r\\nand romance, but in the world outside those books, it is the early 1970s. Everybody says “do your own thing” and “if it feels good, do it. ” My own\\r\\nmother tells me I should never depend on a man to support me: “Go to col-\\r\\nlege, be self-sufficient, so when you do marry, you won’t be financially\\r\\ndependent. ” I agree wholeheartedly, but I am swept away by the romance in\\r\\nmy books, which tell me, like Jo March’s mother tells her, that “to be loved\\r\\nand chosen by a good man is the best and sweetest thing that can happen to\\r\\na woman” (118). I am calling myself a feminist at fourteen, buying Ms. mag-\\r\\nazine, reading Our Bodies, Ourselves, and watching the horizon for the appear-\\r\\nance of Mr. Rochester. Nietzsche was right; all writing is autobiographical in some sense,\\r\\nregardless of the explicit subject matter and regardless of whether we call it\\r\\nfact or fiction. We write to extend our love affair with an idea or image, or we\\r\\nwrite to understand something we have experienced, directly or indirectly. The author of a book on “geek culture” tells a radio interviewer that in his\\r\\nyouth he was smart and different, and thus ostracized. His book about the\\r\\nsubculture of technologically savvy “geeks” is a recovery effort aimed at res-\\r\\ncuing the image of people like himself in mainstream culture.1T. S. Eliot cer-\\r\\ntainly wrote in part to prove his worthiness in elite British literary circles with\\r\\nwhom he desperately wanted to be associated. Virginia Woolf’s anger at her\\r\\nexclusion from membership in that same club is woven into much of her\\r\\nprose.2Simone de Beauvoir’s massive study, The Second Sex, was called into\\r\\nbeing by its author’s knowledge of her own status as Other. And my study is\\r\\nno different.3Its origins are in my own coming of age and the confusion I felt\\r\\nabout what it means to be a woman. Faced with conflicting narratives of\\r\\nwomanhood, none of which are explicitly spelled out, I committed many\\r\\nsocial blunders but eventually (well into adulthood) reconciled myself to\\r\\nbeing a walking contradiction. Still, there was—and is—great pressure on all\\r\\nof us to create order out of chaos and to present ourselves coherently. The\\r\\nworld wants to know how to interact with us, and it does not know how to\\r\\naddress itself to individuals who signal multiple or oppositional identities. The imperative to develop a coherent identity that meshes with society’s\\r\\n2\\u0001From Girl to Woman\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"1497e329-c523-4308-b5cd-c2f04631cfba":{"__data__":"{\"id_\":\"1497e329-c523-4308-b5cd-c2f04631cfba\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"essay.pdf||j+D09K2bvXXJsI4Lyr2pTE7NhdSvCTr5oiijxwDd4GQ=\",\"metadata\":{},\"hash\":\"vAAj9Z/qxMAgzrX+lRtWqxzEhx0iXRzcrwMhUakG0Fo=\"}},\"hash\":\"A0rKv5I09+AxVl3t8XxEdIm/tWjIpotRVkjSmOMN56c=\",\"text\":\"dominant ideologies is unspoken, but individuals who transgress cultural\\r\\nnorms are quickly brought into line or ostracized. This dynamic reaches crit-\\r\\nical mass in adolescence, when individuals in the West are tacitly expected to\\r\\ncoalesce their identities and complete the journey to a unified selfhood. Given the social pressure to conform, I began to wonder how an individual\\r\\ncould find the courage and power to resist culturally sanctioned roles. More\\r\\nspecifically, I wanted to understand how girls become women and how they\\r\\ncope with the conflict between their own desires and the social repression of\\r\\nwomen’s desire. Though I came of age as the second-wave women’s move-\\r\\nment was gaining a foothold in American culture, I did not then believe that\\r\\nthese conflicts mattered in any significant sense. Born too late for conscious-\\r\\nness-raising groups, and too early to take our equality for granted, women of\\r\\nmy generation, race, and class assumed we would go to college and “do”\\r\\nsomething, but we struggled with conflicting ideologies of femininity. We did\\r\\nnot hide our achievement around boys, but we were thought pushy if we took\\r\\nthe initiative in dating. We got the culture’s messages about free love, but\\r\\nwhen we acted on them we were sluts. We were expected to excel, but qui-\\r\\netly and passively. Worst of all, we did not talk to each other about these\\r\\nissues in the way that women ten years older than us did, as serious discourse\\r\\nabout the place of women in Western society remained cloistered in rela-\\r\\ntively remote (from our adolescent lives) locations. Like our mothers, we did\\r\\nnot know that other girls felt the same confusion and discontent that we did,\\r\\nleaving many of us convinced that we were aberrations. My mother enrolled in college when I was coming of age, and I watched\\r\\nas she transformed into a blue-jean-wearing undergraduate who plunged into\\r\\nthe history of American slavery because she had been profoundly moved by\\r\\nthe civil rights struggle as it took shape in the late 1940s and 50s, when she\\r\\nwas coming of age. Still unable to resist a book, I read much of what she read. The Autobiography of Malcolm X. Manchild in the Promised Land. Soul on Ice.I\\r\\ncould not have articulated it then, but I was drawn to these texts because\\r\\nthey showed me that I was not the only one who felt out of step with my cul-\\r\\nture. More than that, I learned that there were literally millions of alienated\\r\\nhuman beings in America who, for a variety of reasons not of their own\\r\\nmaking, did not flourish in mainstream society. But their stories—and my\\r\\nown—stayed on the sidelines of the American master narrative. The autobi-\\r\\nographical texts that form the basis of my argument here thematize the cul-\\r\\ntural alienation and confusion that continue to resonate for me, leading me to\\r\\nask how writers are able to overcome the socially determining limits faced by\\r\\nAmerican women. Identity and the Coming-of-Age Narrative\\u00013\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"67597179-519d-495c-9849-f3777fcff97f":{"__data__":"{\"id_\":\"67597179-519d-495c-9849-f3777fcff97f\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"essay.pdf||8uO5RtoMt6K1rDdWpLDf1J3FyomO4y6vYAr8W8gwAcM=\",\"metadata\":{},\"hash\":\"2Agysaxlh+Y8l1FVBxJzmhAmpPdKUad5RtBuU/jdqwo=\"}},\"hash\":\"xluzNpGbY2zBi4dqm6kY+6w5BtYyYoZadmGn9pvmQ6w=\",\"text\":\"Stories of resistance and difference have always been in circulation, but\\r\\nthey are typically silenced in public discourse because they fail to invoke the\\r\\nuniversal subject of liberal humanism. In general, narrative knowledge is\\r\\ndelegitimated in Western societies when it fails to conform to the empirical\\r\\nknowledge that has been valorized historically by elite white males. Thus,\\r\\nthe stories and experiences of everyone else have been suppressed by those\\r\\nwho control the terms and conditions of scientific discourses. But as Jean-\\r\\nFrançois Lyotard argues, the rules of empirical knowledge are established in\\r\\na circular, self-referential fashion, so finally “there is no other proof that the\\r\\nrules are good than the consensus extended to them by the experts” (29). While the knowledge that is disseminated through narration is often dis-\\r\\nmissed by dominant ideology, for women and other socially marginalized\\r\\ngroups traditionally excluded from socially sanctioned forms of learning, nar-\\r\\nrative has been the primary means of sharing the knowledge that is necessary\\r\\nfor survival. Lyotard calls narration the “quintessential form of customary\\r\\nknowledge” in that it inscribes a society’s criteria for cultural competence,\\r\\nand allows its members to determine the validity and performance of any\\r\\ngiven narrative (19–20). By means of narrative and anecdote, for instance,\\r\\nwomen have shared their unquestionably empirical knowledge of childbirth\\r\\nwith each other and subsequent generations, though the validity of that\\r\\nknowing has often been ignored or dismissed in male-sanctioned medical\\r\\ndiscourse. In the West, we have privileged the consensus of empirical experts’\\r\\nknowledge at least since the Enlightenment, and this consensus dismisses\\r\\nnarrative knowledge as unverifiable and therefore worthless. Lyotard con-\\r\\ntends that narration helps create and sustain social bonds, unlike scientific\\r\\nknowledge, which is “no longer a direct and shared component of the bond”\\r\\n(25). Certainly, it is more than mere coincidence that groups who rely on nar-\\r\\nrative to create knowledge have been constructed as primitive or inferior by\\r\\ngroups that seek to control knowledge by subjecting it to scientific proofs. Oral cultures and groups, traditionally excluded from elite education, have\\r\\nbeen subjected to the cultural imperialism of scientists who dismiss narrative\\r\\nknowledge as unverifiable through argumentation or proofs, and therefore\\r\\nnot genuine. Such cultures are subsequently classified, according to Lyotard,\\r\\nas different: “savage, primitive, underdeveloped, backward, alienated, com-\\r\\nposed of opinions, customs, authority, prejudice, ignorance, ideology. Narra-\\r\\ntives are fables, myths, legends, fit only for women and children. At best,\\r\\nattempts are made to throw some rays of light into this obscurantism, to civ-\\r\\nilize, educate, develop” (27). Lyotard argues, however, that scientific and nar-\\r\\n4\\u0001From Girl to Woman\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"ae1e3cdc-9780-4e35-bc5e-289c158a5026":{"__data__":"{\"id_\":\"ae1e3cdc-9780-4e35-bc5e-289c158a5026\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"essay.pdf||I8IRYbaHIQZkDh0aDwFb3zKPwJ8w9xrzHIMvept79R8=\",\"metadata\":{},\"hash\":\"ypymveoDsCOjEJLD4tXQJTTWIMiQ63xoEzH3A98F6Vs=\"}},\"hash\":\"AH+UeyTPPzKrJNSkLTPtvhElC2GNoF/vrsk/6EPU/Gs=\",\"text\":\"rative knowledges are each judged by a separate set of governing rules, and\\r\\nthat the validity of one kind of knowledge cannot be judged by the rules of\\r\\nthe other (26). Moreover, Lyotard correctly contends that scientific knowl-\\r\\nedge cannot prove its superior position as the “true knowledge without\\r\\nresorting to the other, narrative, kind of knowledge, which from its point of\\r\\nview is no knowledge at all” (29). In other words, in addition to presenting\\r\\nthe requisite proofs and argumentation, scientific discourse—ironically—\\r\\nnecessarily includes narration as it builds a case for its own superiority. Addi-\\r\\ntionally, to qualify as true learning, scientific knowledge must present a\\r\\ncoherent narrative without gaps or contradictions that might be construed as\\r\\nfalse proofs or faulty argumentation, and in this sense, scientific knowledge\\r\\nenjoys a symbiotic relationship with liberal humanist selfhood, which posits\\r\\na similarly seamless account of the autonomous individual. But, as Lyotard\\r\\ncontends, echoing John Donne’s 1624 Devotion, “no self is an island; each\\r\\nexists in a fabric of relations that is now more complex and mobile than ever\\r\\nbefore” (15).4\\r\\nWomen have always understood identity as complex and intercon-\\r\\nnected, and we have historically relied on narrative to convey our contradic-\\r\\ntory, contingent truths and to foster that human connectedness. As Alison\\r\\nJaggar wryly suggests, women never would have devised the liberal human-\\r\\nist account of identity with its conception of individual autonomy and its val-\\r\\norization of the mind over the body which sanctioned a division of labor that\\r\\nallowed a few elite males to concentrate on mental activity while everyone\\r\\nelse attended to the quotidian, physical necessities of everyday life. Further-\\r\\nmore, argues Jaggar, “[i]t is even harder to imagine women developing a\\r\\npolitical theory that presupposed political solipsism, ignoring human interde-\\r\\npendence and especially the long dependence of human young” (46). The\\r\\nstory of human connection in Western societies, “the other side of the story”\\r\\nin Molly Hite’s phrase, has been told by women and, for most of history, in\\r\\nprivate. Furthermore, the relatively few women’s narratives that did gain\\r\\npublic recognition have been viewed as if they were merely a mimetic trans-\\r\\nfer of the author’s life into text (Hite, Other Side13). As Michel Foucault has\\r\\nargued, every member of a society knows the “rules of exclusion”—the pro-\\r\\nduction of discourse is at once controlled, selected, organized and redistrib-\\r\\nuted according to a certain number of procedures, whose role is to avert its\\r\\npowers and its dangers, to cope with chance events, to evade its ponderous,\\r\\nawesome materiality” (“Discourse” 216). Because women’s discourses were\\r\\npotentially dangerous to the continued dominance of Western liberal ideol-\\r\\nogy, women’s narratives and texts have been carefully regulated to allow for\\r\\nIdentity and the Coming-of-Age Narrative\\u00015\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"1322b5d4-acd3-4865-a45a-c8074862f3dd":{"__data__":"{\"id_\":\"1322b5d4-acd3-4865-a45a-c8074862f3dd\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"essay.pdf||+dKnlkpfisOY1lBGC3O0heom5/B3/34cC7OEUM1p9NM=\",\"metadata\":{},\"hash\":\"ddsVeiVLgvoWVZmAPbb1Yxk94RqYQ8lxmlSHk2U7+ig=\"}},\"hash\":\"ko7pXKs13Nfs/bwpDehIImw5H/TyZDcFCp47uzkUn6U=\",\"text\":\"inclusion only those accounts that confirm dominant ideology. The medieval\\r\\ndefinition of madness exemplifies Foucault’s understanding of how exclu-\\r\\nsionary practices work: “a man was mad if his speech could not be said to\\r\\nform part of the common discourse of men. His words were considered null\\r\\nand void, without truth or significance” (216). We need only substitute the\\r\\nword “woman” for “man” in Foucault’s formulation to find a profound\\r\\ndescription of the place of women in Western culture, for while a madman\\r\\nmight reverse his exclusion by reinscribing dominant ideology, a woman who\\r\\nshifts course and voices the same discourse is still “just” a woman. Her dis-\\r\\ncourse can never be that of the “common discourse of men. ” For Foucault,\\r\\ndiscursive exclusion relies on institutional reinforcement from schools, the\\r\\npublishing industry, and other practices to police its borders and “exercise a\\r\\nsort of pressure, a power of constraint upon other forms of discourse” (219). The exclusion of women’s experiences and points of view from public\\r\\ndiscourses is the direct result of these institutional pressures that have\\r\\nsilenced the experiences of women and other marginalized groups. Women’s\\r\\nknowledges remained largely underground, dispersed, because they were\\r\\npublicly constructed as insignificant for failing to reflect universal truths. And\\r\\nwhat was true for women’s experiences was, in turn, reflected in social atti-\\r\\ntudes toward the experience of girls. Nothing in female experience resonated\\r\\nwith the common discourse until very recently in history, and so it has been\\r\\ndefined as insignificant. Now, at the dawn of the twenty-first century, we might agree with James\\r\\nWeldon Johnson that a great deal has been said about “the heart of a girl” as\\r\\nshe makes the transition into womanhood. We have witnessed nothing short\\r\\nof a revolutionary shift in paradigm about women in American society in the\\r\\nlast forty years as nearly every assumption about women has undergone\\r\\nintensive scrutiny, questioning, and reassessment. But in 1912, when The\\r\\nAutobiography of an Ex-Coloured Man appeared, the details and meanings of a\\r\\ngirl’s passage from childhood to womanhood held significance primarily in\\r\\nthe circumscribed private domestic sphere inhabited by women. Denigrated\\r\\nfor lacking universal values and consigned to the sentimental realm, literary\\r\\ntexts focused on girls’ coming-of-age issues received scant public or scholarly\\r\\nattention before the final decades of the twentieth century.5A great many\\r\\nassumptions lie at the core of this ancient silence; the idea that woman is an\\r\\nuncomplicated being whose life course is dictated by biology and nature has\\r\\nbeen argued and defended by thinkers from Aristotle to Freud and beyond. Indeed, this is themaster narrative of womanhood in Western society and it\\r\\ncarries the weight and authority of more than two thousand years of philo-\\r\\n6\\u0001From Girl to Woman\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"16150cba-cff9-49c2-9ba0-3390bd9ce96f":{"__data__":"{\"id_\":\"16150cba-cff9-49c2-9ba0-3390bd9ce96f\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"essay.pdf||Z/fmghpU+nLad+Ro9mun6AQ4k0P8egZ4pgIcq6TYgPE=\",\"metadata\":{},\"hash\":\"AcvYCu9SXnlz92d0Jah08uEfukk5OWA3DsNccB4dwo8=\"}},\"hash\":\"QMcd9lXFvcVwa055RRhiJad/dOmU4ipYq0radpo9yw0=\",\"text\":\"sophical and scientific traditions. Yet in spite of that tradition, the lives of\\r\\nAmerican women have changed more radically in the last forty years than in\\r\\nany other comparable period in history. This sea change became possible\\r\\nbecause women not only imagineda changed social landscape, in Raymond\\r\\nWilliams’s terms, they also took action to overcome the “determining limits”\\r\\nof white patriarchal capitalist hegemony (86). In Carolyn G. Heilbrun’s view,\\r\\nthe task of her generation of feminists has been more “to dismantle the past\\r\\nthan to imagine the future” (72), but articulation of women’s lives under\\r\\npatriarchy could not avoid imagining alternate ways of being.6The work of\\r\\nimagination and deep social change has reached every sector of American life\\r\\nwhere women exist, from the small consciousness-raising groups of the 1960s\\r\\nto organized activist groups to rigorous scholarship in the academy, and while\\r\\nthere continues to be vocal, sometimes violent resistance to the increased\\r\\nsocial and political power women wield, there is little question that feminism\\r\\nhas succeeded in changing American culture. To all appearances, this\\r\\nwomen’s movement has created lasting social change. Even the most strident\\r\\nenemies of feminism acknowledge by their energetic attacks its far-reaching\\r\\nimpact and influence—even acceptance—but I find myself returning again\\r\\nand again to the narrator of The Autobiography of an Ex-Coloured Manand his\\r\\ncomparison of male and female coming-of-age experiences. It might be easy\\r\\nto dismiss his statement as a typically male view of womanhood, but I wonder\\r\\nif Johnson has in fact articulated what women themselves feel. Little Women’s\\r\\nJo March wants desperately to be a boy because she understands that adven-\\r\\nture and power belong to males while women must be content with “work-\\r\\ning and waiting.”7But even in the twentieth century, Maxine Hong Kingston\\r\\nwrites of her longing to be a boy, that her parents might take pride in her\\r\\nachievements. I can dismiss the first part of Johnson’s statement—“A great\\r\\ndeal has been said about the heart of a girl when she stands ‘where the brook\\r\\nand river meet’”—as the dubious assertion of an earlier age, but it is much\\r\\nmore difficult to dismiss what immediately follows in Johnson’s text: the nar-\\r\\nrator’s assertion that “what she feels is negative” puzzles and angers me\\r\\nsimultaneously. Do most girls feel negatively about becoming women? What\\r\\nis the cause and nature of that negativity? What does it mean to become a\\r\\nwoman in twentieth-century America? These are the foundational questions of my study, and while I cannot\\r\\nclaim to answer them empirically or definitively, I have undertaken the work\\r\\nin part to take issue with the continued devaluation of girls and women in\\r\\nAmerican culture, which also grounds the implicit assumption in some femi-\\r\\nnist theory that a negative view of womanhood by women is “natural” and\\r\\nIdentity and the Coming-of-Age Narrative\\u00017\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"a0aa49aa-c043-4c19-88da-792eda43c5a7":{"__data__":"{\"id_\":\"a0aa49aa-c043-4c19-88da-792eda43c5a7\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"essay.pdf||QukymsQibVbJyCoYH6SuN55rjl/sGCTwHIZfBIKrtjs=\",\"metadata\":{},\"hash\":\"ZCoTcwFFf6IVETEeE5+l9zg2dFhM+zZa3VAGIngbBmk=\"}},\"hash\":\"i3Qx9xRTnYGIxfWqxDYWROdKgUA0N/1FHMYk/6t47+c=\",\"text\":\"understandable given the many restrictions and the suppression of self that\\r\\nare ostensibly unavoidable when a girl comes of age. While it has been polit-\\r\\nically necessary and expedient (in order to force social change) to highlight\\r\\nthe ways and means of women’s oppression, chapter and verse, here I will\\r\\nargue that in spite of the unquestionably repressive culture faced by Ameri-\\r\\ncan women of all backgrounds throughout history, female writers of what I\\r\\ncall the coming-of-age narrative resist negative constructions of womanhood\\r\\nand actively create oppositional identities for themselves. In order to create\\r\\nsuch an identity, however, the writer must also reject conventional literary\\r\\ngenres, whose ideologies make the construction of alternative identities\\r\\nimpossible.8Instead, women writers created a specific type of text—the\\r\\ncoming-of-age narrative—that subverts traditional literary forms in order to\\r\\nconstruct new forms of subjectivity and resist the male-defined discourse of\\r\\nwomanhood.9While it would be naive to ignore the ambivalence with which\\r\\nmany girls have approached a future of performing a socially acceptable\\r\\nAmerican womanhood, which Annie Dillard compares to a living death, in\\r\\nthe coming chapters I will argue that girls do not suffer gladly the inevitable\\r\\nmarch to their socially mandated adult roles. As powerful as the master nar-\\r\\nratives might be, they are riddled with contradictions and outright lies, a fact\\r\\nrecognized and exploited by female American memoir writers as they resist\\r\\nhegemonic interpellations and attempt to claim a degree of agency while still\\r\\nacknowledging the ineluctable social contexts that help determine their\\r\\nidentities. Moreover, it is apparent that women who write coming-of-age nar-\\r\\nratives construct these oppositional subjectivities only in retrospect. In other\\r\\nwords, the act of writing one’s coming-of-age experience is also the act of\\r\\nordering the conflicts and confusions—even chaos—related to the construc-\\r\\ntion of identity in adolescence, a feat not easily accomplished in medias res. Women who write coming-of-age narratives construct discursive selves\\r\\nactively engaged with American ideologies of womanhood in its myriad man-\\r\\nifestations. In addition, these texts violate the traditional boundaries of auto-\\r\\nbiography and fiction by subverting the reader’s desire for coherent narratives\\r\\nthat clearly signal their status as either truth or fiction and that will reinscribe\\r\\nand verify a unified selfhood. By fusing and blending narrative devices, these\\r\\ntexts use language and the act of narration to challenge hegemonic construc-\\r\\ntions of identity and womanhood, and create a form of what Catherine Belsey\\r\\nhas termed the “interrogative” text, which positions the author as a contin-\\r\\ngent, contradictory subject and raises more questions than it answers for the\\r\\nreader (91). While realist texts present a fixed identity that reinforces the\\r\\nreader’s identification with the universal subject, the interrogative text calls\\r\\n8\\u0001From Girl to Woman\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"f8d24ac5-62a9-4a08-b325-d67ba54fa5c2":{"__data__":"{\"id_\":\"f8d24ac5-62a9-4a08-b325-d67ba54fa5c2\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"essay.pdf||/QanAuJ+7y0ZjTptqVeO8pbeXFT2gpW6Ob/LOoZSvO0=\",\"metadata\":{},\"hash\":\"q5z96uNwEHIfujpRKaOGstNlKwry6iEF82sfB7W4PEo=\"}},\"hash\":\"fjLSUAsH/EoZ63lWNKyiAbgWiAxzJBhRW0b5hxMN2TE=\",\"text\":\"forth a reader who is similarly alienated from dominant discourse and who\\r\\nidentifies with a fluid, often contradictory subject position. As Leigh Gilmore\\r\\nwrites of postmodern autobiographical practices, these texts constitute a “site\\r\\nof identity production . . . that both resist and produce cultural identities” (4). Moreover, these sites of identity production are constantly shifting, and priv-\\r\\nilege contingent subjectivities by refusing to repress discourses that contra-\\r\\ndict the assertion of autonomous selfhood. The coming-of-age narrative\\r\\nforegrounds the pain and confusion that accompanies a conflicted subject\\r\\nposition—we all want to belong, somehow, to a culture that recognizes only\\r\\nthe coherent subject as normal—but these texts argue that normality is, in\\r\\nthe end, chimerical. And that insight is finally productive of power and\\r\\nagency for women. By focusing on adolescence, by definition a time of rebel-\\r\\nlion and resistance, and by foregrounding contradictory desires and dis-\\r\\ncourses, the coming-of-age narrative provides a congenial form for women\\r\\nwriters to successfully question the power of dominant ideologies to con-\\r\\nstruct their lives. The proliferation of second-wave feminism, as well as critical theory’s\\r\\nmovement toward poststructuralism, has helped to encourage and validate a\\r\\nmassive increase in the publication of women’s narratives. However, with\\r\\nfew exceptions, previous studies implicitly view women (to recast Sartre’s\\r\\nfamous critique of Marxism) as if their lives began with their first romance,\\r\\nor with marriage. Even those studies that focus on the female Bildungsroman,\\r\\nsuch as Rachel Blau du Plessis’s work on the narrative strategies of twenti-\\r\\neth-century women writers and Barbara White’s work on fictional represen-\\r\\ntations of female adolescence, often focus on the teleology of adolescent\\r\\nquest, rather than with the childhood that preceded it. Indeed, the most\\r\\ninfluential studies to date have mainly been concerned with fictional repre-\\r\\nsentations of women’s lives, and use autobiographical and other nonfiction\\r\\ntexts of women writers primarily to bolster their claims. Furthermore, previ-\\r\\nous studies have argued that quest and romance are rendered mutually\\r\\nexclusive in women’s literature, and implicitly suggest that romance always\\r\\nforecloses quest. In contrast, many coming-of-age narratives refuse the\\r\\neither/or opposition, insisting instead that romance andquest are entirely\\r\\ncompatible, and thus valorize boththe self-in-relation and individual quest. My intention is to broaden the scope of literary and cultural analysis of\\r\\nwomen’s lives by focusing on the transition from childhood to womanhood\\r\\nas it is constructed in autobiography and memoir. And because there is a\\r\\ndeeply ingrained tendency to read autobiography as unproblematically\\r\\ntruthful, I make extensive use of feminist poststructuralist theory, which\\r\\nIdentity and the Coming-of-Age Narrative\\u00019\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"b6d30409-8c58-4d0f-a767-dcb90d312c83":{"__data__":"{\"id_\":\"b6d30409-8c58-4d0f-a767-dcb90d312c83\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"essay.pdf||pEUEspFmpXDtZzpPZCWHQYbgoI0q+cJT4La12LHuhzI=\",\"metadata\":{},\"hash\":\"vshsGLKCwT6drgZWkpCAOHl5QLThz5baV+XDjTdh8tI=\"}},\"hash\":\"I4rCiu2j2yCgUx/FP4QVk0X/F8GxudiI8B8wVGJ5ibo=\",\"text\":\"suggests that, while experience plays a major role in determining what sort\\r\\nof woman the child becomes, what is critical is how she creates meaning\\r\\nfrom those experiences from the discourses available to her when she writes. The Freudian narrative of so-called normal femininity is regularly chal-\\r\\nlenged in twentieth-century coming-of-age narratives, as is the conception\\r\\nof an essential womanhood (defined as self-in-relation) offered by liberal\\r\\nfeminist interventions such as that of Belenky, et al.While these texts do\\r\\nlend credence to poststructuralist theories of subjectivity as a process, they\\r\\nrepeatedly assert the primacy of experience as producer of truth. But truth\\r\\nhere is a local and specific truth; few female autobiographers in any histori-\\r\\ncal era have rhetorically suggested that theirs are universal stories. Further-\\r\\nmore, most of the writers in my study tacitly recognize the unstable nature\\r\\nof truth. These texts demonstrate how twentieth-century autobiography consti-\\r\\ntutes a sphere in which the dominant American ideologies of womanhood are\\r\\nfrequently and radically challenged and profoundly revised. While I do argue\\r\\nthat all writing is autobiographical in some sense, and further that texts iden-\\r\\ntified as autobiography are nonetheless fictions constructed by the writer to\\r\\nmake “sense” of her life, these narratives present a far more complex picture\\r\\nof female gender formation and practices than that found in most fiction pre-\\r\\ncisely because of the ideological restrictions of fictional forms. And while I\\r\\nwould agree with du Plessis that the twentieth century has seen a shift in the\\r\\nteleology of women’s lives as depicted in fiction, I argue further that many\\r\\nautobiographies go beyond simply imagininga different life trajectory for\\r\\nwomen—they show women livingit, full of contradictions, but effectively\\r\\nresisting society’s scripts for women. RECREATING WOMANHOOD\\r\\nMy goal is to build on a growing body of scholarship that deals with the con-\\r\\nstruction of womanhood by analyzing the ways in which subjectivity and\\r\\nidentity10are constructed in twentieth-century American coming-of-age nar-\\r\\nratives by women. While these texts do provide evidence that growing up\\r\\nfemale in America is still a sometimes painful experience, they also demon-\\r\\nstrate a great diversity of experience, much of which is joyful and gives lie to\\r\\nthe considerably flattened and simplistic view of women’s lives expressed\\r\\nmost memorably by Sigmund Freud in his exasperated question: “What do\\r\\nwomen want? ” Freud arrogantly answered his own question at great length,\\r\\n10\\u0001From Girl to Woman\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"44ebb065-39b9-4308-ada5-32eacc0e96fe":{"__data__":"{\"id_\":\"44ebb065-39b9-4308-ada5-32eacc0e96fe\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"essay.pdf||FeyRGTbuLhwCOCYxXZhHpwd6nuub/T10TFTNHIDJin8=\",\"metadata\":{},\"hash\":\"MS9vSCE4AjINrFu/a93t4YV5UolISLSfE1I5Oj7aWVE=\"}},\"hash\":\"ul9sHNNOZRYnrX9Blu+VIIHbw1hPV3e5wtc7HvBs3Pk=\",\"text\":\"assuming he was more qualified than women themselves to answer the ques-\\r\\ntion; Virginia Woolf imagined Freud writing on women as “labouring under\\r\\nsome emotion that made him jab his pen on the paper as if he were killing\\r\\nsome noxious insect as he wrote, but even when he had killed it that did not\\r\\nsatisfy him; he must go on killing it” (31), and contends that only by calling\\r\\nwomen inferior could he see himself as superior (35). Women’s own narra-\\r\\ntives of coming of age answer Freud’s question quite differently than he did;\\r\\nany acceptance of a “lack” based on gender is entirely missing from these\\r\\nmemoirs, although their authors certainly recognize that, as women, they are\\r\\nperceived as inferior by American society. Instead, these narratives often\\r\\nreflect the widening horizons for women in American culture and a strong\\r\\nresistance to normative femininity; female quest takes many forms, and suc-\\r\\ncessful resistance to gender norms becomes possible and even acceptable, if\\r\\nstill sometimes unconscious or covert. Du Plessis argues that when women as\\r\\na group are successful in questioning social gender norms, female quest nar-\\r\\nratives in novels will no longer routinely resolve in marriage or death for the\\r\\nheroine (4). In the memoirs I examine in the chapters ahead, there is a con-\\r\\nsistent thematization of quest in lives that span the twentieth century, but\\r\\nhere, unlike in the novels discussed by du Plessis, narrative choices are not\\r\\nfinally limited by the romance plot. I suggest then that the narrative choices\\r\\nin fictional accounts of women’s lives have been far more constricted than\\r\\nthose found in women’s coming-of-age narratives. Women autobiographers are no freer from gender norm pressures than\\r\\ntheir fictional counterparts, but their narratives resist the suppression and\\r\\nlimitations of their opportunities and choices, and this resistance is ultimately\\r\\nproductive. In addition, the arguably artificial boundaries between fiction\\r\\nand autobiography produce a paradoxical reader reaction; we are less forgiv-\\r\\ning of seemingly unlikely events or attitudes in fiction, but in a nonfiction\\r\\ntext there is a greater suspension of disbelief, allowing the writer greater lat-\\r\\nitude in the construction of truth. Of course, in real life, preposterous (or what\\r\\nNancy Miller would call “implausible”) events and coincidences do indeed\\r\\noccur, but as readers we have more structured expectations of fictional plot\\r\\nmovement and character behavior.11This phenomenon may explain why\\r\\nfemale autobiographers construct themselves as less restricted in life choices\\r\\nthan their fictional counterparts, even at this late date. Although she did not\\r\\nwrite an autobiography, Louisa May Alcott will serve as an example of this\\r\\npoint. Alcott never married, choosing instead to support her parents and sis-\\r\\nters through her writing, while she does not (or cannot) allow her fictional\\r\\nalter-ego, Jo March in Little Women, to break with social norms. Indeed, Jo is\\r\\nIdentity and the Coming-of-Age Narrative\\u000111\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"0e1f51a7-8e39-41be-97ee-3a5ac4097a21":{"__data__":"{\"id_\":\"0e1f51a7-8e39-41be-97ee-3a5ac4097a21\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"essay.pdf||/jkjEGY6UxdUsTMGTf7YglD20FCJfpQanlKHMiuy6cY=\",\"metadata\":{},\"hash\":\"FBn9Yr11OXl3mwmsL3yz/g+jIV7HPXXKCIrg2TtW9/c=\"}},\"hash\":\"h0IQ+ECvLHEWr3v8B5cODaNu/t2f0RqyLJ1wp/3JTAY=\",\"text\":\"persuaded to give up her career of writing adventurous “sensation” stories so\\r\\nthat she may become a more suitable wife in her fiancé’s eyes. Alcott’s life\\r\\nwas more complex than this brief rendering suggests, but the point is that\\r\\nwhile women might commonly live lives that do not measure up to the\\r\\nmythic norm, their fictional narratives suggest much less room to negotiate\\r\\ngender norms than actually exist. While this might suggest the writers are\\r\\nexhibiting classic false consciousness, I see this pattern as a example of\\r\\ncounterhegemony at work. Raymond Williams defines hegemony as a com-\\r\\nplex concept of political and cultural dominance, gained through an ongoing\\r\\nprocess of consent,12and argues that the window for resistance to hegemony\\r\\nlies in the imagination of individuals (86). Thus, by imagining an altered view\\r\\nof the self, the individual reclaims agency and resists society’s interpellation. And, as Barbara Bellow Watson contends, women’s literature enacts the\\r\\nabstract politics of womanhood through highly specified characters, contexts,\\r\\nand meanings (112). The writers considered here do not consciously set out\\r\\nto reimagine womanhood, but in carving out space for alternative subjectivi-\\r\\nties, they are rewriting the social scripts allotted to women. My purpose then is to examine the ways women from a variety of back-\\r\\ngrounds construct their subjectivity in their coming-of-age narratives. Fol-\\r\\nlowing a theoretical thread on the nature of subjectivity beginning with\\r\\nHegel and Marx and continuing through the feminist poststructuralist revi-\\r\\nsions offered by critics such as Chris Weedon and Patricia Waugh, I argue that\\r\\nthe texts of women’s coming-of-age narratives contest the tenacious hold of\\r\\nthe liberal humanist notion of self on Western notions of subjectivity. Implic-\\r\\nitly and explicitly, these texts assert that the self does not exist outside of lan-\\r\\nguage, historical context, or culture, and they echo Simone de Beauvoir’s\\r\\nassertion that women are made, not born (301).13Yet, coming-of-age narra-\\r\\ntives also assert the embodiedness of identity, often through an apparently\\r\\nunconscious search for bodily knowledge. The physiological changes of\\r\\nfemale puberty seem to work against the cultural pressure to ignore the body,\\r\\nresulting in a notable bodily presence in the narratives I examine here. A\\r\\nwoman’s understanding of her identity and how she came to be the person\\r\\nshe is, as related in these narratives, generally resists socially determined\\r\\nroles and life trajectories, if sometimes in less than forthright ways. This is in\\r\\nmarked contrast to women’s writing of the eighteenth and nineteenth cen-\\r\\nturies, when the hegemony exerted great pressure on girls who longed to\\r\\n“jump at the sun,” in Zora Neale Hurston’s phrase (Dust 21), to come back\\r\\ninto line and fulfill their womanly destiny as defined by social and cultural\\r\\nnorms.14Further, the texts I consider here refuse the overdetermined sub-\\r\\n12\\u0001From Girl to Woman\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"47c37987-d689-4f7d-af71-50812fd50231":{"__data__":"{\"id_\":\"47c37987-d689-4f7d-af71-50812fd50231\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"essay.pdf||/y2IZVS31txDAjm3H617Sdpz14hB8T72qfsienrU4ek=\",\"metadata\":{},\"hash\":\"0l3uW0FjqmcaRmkteU6bJEAQbbNBfI0Kx59MyJAJdjU=\"}},\"hash\":\"c3HCiXeig7xT2gZVXryTaGCwoFusZrzH/u4DrsVm+lQ=\",\"text\":\"jectivity suggested by Louis Althusser’s theory of interpellation by ideologi-\\r\\ncal state apparatuses (ISAs) as well as later poststructuralist assertions that\\r\\npronounced the “death of the author. ” Considered as a body of texts, twen-\\r\\ntieth-century women’s narratives argue that it does indeed matter who wrote\\r\\nthe life being examined and that the writer’s agency is a given while simul-\\r\\ntaneously acknowledging that gender and other subject positions are socially\\r\\nconstructed. This view is no less ideological than the one that burdened pre-\\r\\nvious generations of women writers, but it does modify the strict opposition\\r\\nof individual agency and constructed identity, a significant move toward the\\r\\nself-determination called for by the women’s movement. American women’s\\r\\ncoming-of-age narratives demonstrate the complexity and infinite diversity\\r\\nof American women’s lives; once we read them, we realize it is nearly impos-\\r\\nsible to essentialize women as long as their stories are told—and heard. Furthermore, these texts evince a complex subjectivity which cannot\\r\\nultimately be reduced to archetypes or to a single philosophical stance. The\\r\\nliberal humanist paradigm of human nature retains a tight grip on Western\\r\\nculture, if these texts are any indication, because most reflect the cultural\\r\\nimperative to fix subjectivity and produce noncontradictory narratives of self-\\r\\nhood. While race and sex theoretically should not figure in the liberal notion\\r\\nof human ‘essence’, in practice these discourses and others do indeed work\\r\\nto fix subjectivity. Some autobiographies will inadvertently encourage a\\r\\nreader to interpret the subject’s identity through a single lens, resulting in, for\\r\\nexample, Elizabeth Wurtzel’s subjectivity seeming to originate in her clinical\\r\\ndepression in Prozac Nation (1994); likewise, Richard Wright’s Black Boy\\r\\nseems to suggest that his subjectivity is determined by race. The implica-\\r\\ntions of their titles aside, Wurtzel and Wright both actively resist narrow iden-\\r\\ntities as, respectively, a “depressive” and a “black boy” with all the cultural\\r\\nbaggage those terms imply. Both seek to deepen their subjectivity through\\r\\nassertions of intelligence and multiple subject positions beyond the ones sug-\\r\\ngested by their titles. However, adolescence—coming of age—is by defini-\\r\\ntion a time when identity is fluid and contradictory, and thus provides solid\\r\\nevidence for the notion of constructed subjectivity. And, as I argue in detail\\r\\nlater, while there is the appearance of fixed subjectivity in the texts, readers,\\r\\nas well as the autobiographers themselves, deconstruct and reconstruct the\\r\\nsubject in light of what Norman Holland calls their own “identity theme”\\r\\n(815). Thus, while there might be a sense of coherent subjectivity on the part\\r\\nof the autobiographer at the time of writing, readers reinterpret the text in\\r\\nlight of the immediate cultural context in which they read, creating addi-\\r\\ntional opportunities for subversive or alternate meanings. Identity and the Coming-of-Age Narrative\\u000113\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"e68e1428-f18e-4d46-a272-ea7a1db0856f":{"__data__":"{\"id_\":\"e68e1428-f18e-4d46-a272-ea7a1db0856f\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"essay.pdf||QcklhPs33xTIMyeacu01w9LS+l+u2+HUTaN6awrz+BQ=\",\"metadata\":{},\"hash\":\"cgIo+rJncSbrjVng/b827dZfNjaxZgM62OYDtqOokyM=\"}},\"hash\":\"aG/WPJ0VD9RJYlt1dt+D3/Ylps6F58aqICnnlY4hQ/o=\",\"text\":\"Although philosophy and the social sciences can provide literary analysis\\r\\nwith useful insights, it is difficult to use social theory unproblematically to\\r\\nanalyze women’s coming-of-age narratives since it tends to describe human\\r\\ndevelopment in prototypes and universals that invariably reflect normative\\r\\nmale development. But even for male development, psychology theory makes\\r\\nbroad assumptions about class, race, and ethnicity, which results in partial\\r\\ninsights and a great many caveats. Nonetheless, I will make limited use of psy-\\r\\nchological theory in the coming chapters to pinpoint the discursive practices\\r\\nthat create the boundaries and teleologies of coming of age in a particular time\\r\\nand place. In part, my analysis is modeled on the aims of contemporary anthro-\\r\\npology, which is to say that my intent is to describe the specificity of cultural\\r\\nexperience through the examination of coming-of-age narratives. These texts\\r\\nare what Clifford Geertz defines as “first-order” interpretations—the creator\\r\\nof the text is playing the part of the “native” in cultural anthropology, and in\\r\\nthat role, she lays claim to being the primary and privileged interpreter of her\\r\\nexperience. But it is important to emphasize that the writer’s role in narrating\\r\\nher own life is an interpretive one; even she has no direct access to the “truth. ”\\r\\nAs a literary critic, then, I am here creating second- or even third-order inter-\\r\\npretations (15). Applying Geertz’s framework of ethnography to the study of\\r\\nthe coming-of-age narrative encourages a healthy, respectful stance to the\\r\\nexercise and, most important, privileges the interpretationof experience ren-\\r\\ndered by the autobiographer. While I will offer an analysis of subject con-\\r\\nstruction in a number of texts, I will attempt to do so from an “actor\\r\\norientation” in order to understand—and interpret—the social, political, and\\r\\nhistorical forces that help the narrator to construct her identity and subjectiv-\\r\\nity in particular ways. Geertz points out that this is ultimately an unattainable\\r\\ngoal, but a necessary foundation for doing ethical and informed ethnogra-\\r\\nphy—and by extension, interpreting narratives of real peoples’ lives. Mindful\\r\\nof Rabinow’s critique of Geertz’s removal of himself as subject in his ethno-\\r\\ngraphic writings, I have provided what I hope is a sufficient sense of the\\r\\n“identity theme” that I bring to my readings of these narratives. I choose them\\r\\n(or they chose me) because, in one way or another, each reflects some aspect\\r\\nof my own experience, so in attempting to make sense of the meaning of the\\r\\ntexts, I am also creating meaning of my own experiences.15\\r\\nHaving defined a distinct genre of American women’s literature that I\\r\\ncall the coming-of-age narrative, I will explore the implications of my argu-\\r\\nment in the context of the current debate on identity politics in the academy. As Linda Martín Alcoff notes, “[t]he constitutive power of gender, race, eth-\\r\\nnicity, sexuality, and other forms of identity has, finally, suddenly, been\\r\\n14\\u0001From Girl to Woman\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"aedfc1e7-6307-47af-8373-c9bc476e7ff5":{"__data__":"{\"id_\":\"aedfc1e7-6307-47af-8373-c9bc476e7ff5\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"essay.pdf||wn/J2PyaEjVQX7jkicfo6si2JGGn+FLGyrcwaN9ORiA=\",\"metadata\":{},\"hash\":\"PGir8rdNOIAx6nx/6CElwT7LyEhULrH+7Kfuk27jlCQ=\"}},\"hash\":\"qis0Ogy2obta7/jKwziruAZIc09DAG8LkJH7uAZVO3c=\",\"text\":\"recognized as a relevant aspect of almost all projects of inquiry” (Who’s), and\\r\\nthus, the fact that the narratives I have discussed here provide evidence of\\r\\nthe social construction of subjectivity is hardly a radical assertion at this his-\\r\\ntorical moment. But I want to argue that, for women writers, the coming-of-\\r\\nage narrative constitutes an alternative literary form that allows the writer to\\r\\nclaim agency in the construction of her identity, while also acknowledging\\r\\nthe role of social discourses in determining subjectivity. Because these texts\\r\\noften deny the distinction between fiction and fact as key to the truth value\\r\\nof a text, the coming-of-age narrative is a genre that resists discourses that fail\\r\\nto describe the writer’s knowledge and experience by subverting the ideo-\\r\\nlogical premises of traditional literary genres. In other words, the conven-\\r\\ntional plots, characters, themes, and ideological bases of fiction and\\r\\nautobiography are revised or erased in the coming-of-age narrative, which\\r\\nallows the writer to inscribe an alternative subjectivity and reclaim agency in\\r\\ndefining herself. The writer’s self-definition arises from direct experience in\\r\\nthe world as well as from the discursive formations specific to her historic\\r\\nlocation, and these narratives strongly affirm the role of experience in iden-\\r\\ntity formation. Arguing against the notion that experience is a direct source of\\r\\ntruth, the coming-of-age narrative demonstrates the ways in which the mean-\\r\\ning of experience is mediated, not only by ideology but also by other experi-\\r\\nence. Identity is thus conceived of as cumulative, with each experience\\r\\nmediated by everything that has come before and subject later to reinterpre-\\r\\ntation in light of new experiences. Furthermore, while I have carefully avoided making generalizations\\r\\nabout women’s lives in my readings of these texts, by locating the coming-of-\\r\\nage narrative in texts across race, class, and time, I implicitly suggest that\\r\\ncommonalities do exist. More specifically, the central ideological assumption\\r\\nof the coming-of-age narrative is that identity is created in the context of\\r\\nhuman relationships. All acknowledge the power and pressure of social\\r\\nnorms, but the specific norms vary according to the writer’s historical and cul-\\r\\ntural location. There is, therefore, no monolithic femininity invoked in the\\r\\ncoming-of-age narrative, but rather, many versions of femininity, which are\\r\\nhighly specified within racial, class, and ethnic discourses. Often, too, a girl\\r\\ncoming of age will have to contend with the discourses of womanhood as it is\\r\\ndefined within her community, as well the definitions imposed from outside. What remains, then, when all the historically and socially specific factors\\r\\neliminate the possibility of essentializing what it means to become a woman\\r\\nin America, is a self that, like other selves, “you find through love and\\r\\nthrough your relations with family and friends” (Culler 115). Identity and the Coming-of-Age Narrative\\u000115\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"28b4a213-6372-4af3-a71e-5704193001c6":{"__data__":"{\"id_\":\"28b4a213-6372-4af3-a71e-5704193001c6\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"essay.pdf||mTqQb2yceI3Otc7/I2+229A97vK67/xknKuvavo8p88=\",\"metadata\":{},\"hash\":\"bYoqbLUJiZ/tcayt4PjvFm6TX+iab/yLuNx8gqsdPk4=\"}},\"hash\":\"7dJlQmecBVuGlm3Q4IHgfQEg491AMY7SB+PLH2e1KEs=\",\"text\":\"The credit for this view of subjectivity and identity formation belongs to\\r\\nfeminist theorists such as Carol Gilligan and Nancy Chodorow, who have been\\r\\nsubject to much criticism in recent years for purportedly essentializing women\\r\\nbut whose description of female development has not only evolved into the\\r\\nmaster narrative of womanhood—it has gained widespread currency across the\\r\\ngender divide as well. Before feminists described the ways in which women\\r\\nconstruct identity, the master narrative of American identity held that the\\r\\nindividual is autonomous and that identity is inherent within each person; the\\r\\nworld of relationships and ideas was irrelevant to the nature of the true self. Imperceptibly, however, the master narrative has radically shifted to valorizing\\r\\nthe self-in-relation to such a degree that it has become hegemonic. Women\\r\\ncontinue to be held accountable for their ability to nurture relationships, and\\r\\nif a woman does not define herself in terms of her relationships (wife, mother,\\r\\ndaughter, friend), she runs the risk of being an “unnatural” woman (or worse). Increasingly, however, public discourse is acknowledging the centrality of\\r\\nrelationships for American men as well. But while American culture periodi-\\r\\ncally suffers spasms of backlash against the “feminization” of selfhood,\\r\\nthrough comic jabs at the “sensitive” male who is in touch with his feminine\\r\\nside, and through the attempts of some feminists to divest themselves of any\\r\\nvestiges of traditional signs of femininity, the discourse has changed. In her\\r\\n1999 study of American men, Susan Faludi argues that the grand narratives of\\r\\nAmerican manhood evinced an opposition between \\r\\nthe vision of the man who stood apart from society and the man who\\r\\nwas a part of society; the loner was not the ideal. The “Indian\\r\\nFighter” was ultimately a homesteader. Daniel Boone was not\\r\\nsimply a tale of a frontiersman taming the world with his rifle and\\r\\nhis knife. Essential to the myth of his journey into the wilderness\\r\\nwas his return from it to retrieve his family and establish a new com-\\r\\nmunity (10). Unable to live according to the myth that has been handed down to them,\\r\\nAmerican men are, according to Faludi, deeply alienated and disillusioned. The myth has worked to isolate men from the people most likely to care for\\r\\nand about them while simultaneously encouraging them to devote their\\r\\n“true” selves to the public world of work. Yet men who have given the best\\r\\nof who they are to faceless corporate entities too often find that their loyalty\\r\\nis not returned. Having thus neglected self-in-relation in favor of self-in-\\r\\nisolation, many American men feel, as Faludi puts it, stiffed. 16\\u0001From Girl to Woman\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"cd677ed3-a582-4887-baaf-c067a62efcef":{"__data__":"{\"id_\":\"cd677ed3-a582-4887-baaf-c067a62efcef\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"essay.pdf||whEERXPHbiOjJRpOIBm8HPTkHYYZJ/cqZYpoe/qrhe8=\",\"metadata\":{},\"hash\":\"yvL8g3YcQuNsWUqjXHxvOrC/g1LuNc+HEyPNPbbaDQ0=\"}},\"hash\":\"fmQKx1BgAHzG+dvwqSFkk9NitNFXe6WjC2goJCk/MEk=\",\"text\":\"Related to the recognition that men have been as alienated as women by\\r\\nthe myths of the autonomous individual is a trend in popular psychology that\\r\\nis concerned with the nurturance of boys in American culture. In his 1999\\r\\nbook Raising Cain, Daniel J. Kindlon argues that boys are harmed by the cul-\\r\\ntural perception that their primary developmental task is to separate from\\r\\nrelationship and establish themselves as independent, self-sufficient individ-\\r\\nuals. As a result, boys are emotionally conflicted because, according to Kind-\\r\\nlon, a great need for connection clashes with a boy’s recognition that he is\\r\\nexpected to sever his dependence on relationships (3).16The current\\r\\nassumption about manhood is that, to be healthy, it must be nurtured through\\r\\na variety of relationships, including those with women, beginning with the\\r\\nmother. Though I do not argue that this account constitutes a universal nar-\\r\\nrative on manhood, it has gained cultural capital in the past twenty years, and\\r\\nthere is little public debate about its merits. Counterhegemonic discourse on\\r\\nidentity formation has thus become hegemonic, and evidence for its domi-\\r\\nnance can be found in the fact that many male coming-of-age narratives\\r\\nwritten in the latter part of the twentieth century thematize the self as con-\\r\\nstructed in relationship. But in an article for The Nation, Patrick Smith critiques what he con-\\r\\nsiders the excesses of contemporary memoirs, calling the worst of them\\r\\n“intellectual frauds” for valorizing the private realm at the expense of what\\r\\nhe considers the much more valuable public realm. Smith produces only\\r\\nwomen’s texts as examples of memoirs that “privatize” history and\\r\\n“refus[e] the challenge of unburying the past as it really was,” and thus he\\r\\nreproduces the ancient critiques of women’s literature. In other words,\\r\\nSmith argues that by articulating the private realm, women writers do “vio-\\r\\nlence” to history. But Smith praises Angela’s Ashes and other (mostly male)\\r\\ntexts for their “dedication  to public discourse. . . or to some object or event\\r\\noutside the self. . . .The power of [Angela’s Ashes] lies in its account of an\\r\\nemerging consciousness—a universal experience that is rarely articulated\\r\\nwell. ” The traditional critiques of women’s texts are, in other words, alive\\r\\nand well, even in ostensibly left-wing periodicals. Smith’s definition of the\\r\\ncorrect purpose of autobiography as that of articulating the “emerging con-\\r\\nsciousness” withinpublic discourse is limited and limiting in its notion of\\r\\nwhat constitutes consciousness, and it is resisted in both male and female\\r\\ncoming-of-age narratives. To suggest that consciousness is significant only\\r\\nin the context of public discourse is to deny the constitutive element of the\\r\\nvastly more influential private world of relationship. The world of the child\\r\\nand adolescent is almost by definition private, and it is during this time that\\r\\nIdentity and the Coming-of-Age Narrative\\u000117\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"f19f7184-9095-4b01-a643-2dee85348df0":{"__data__":"{\"id_\":\"f19f7184-9095-4b01-a643-2dee85348df0\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"essay.pdf||Ft6v52B2K3Z3zf0d+7UenLQpXXF6jTjkl2jqGbOWhlc=\",\"metadata\":{},\"hash\":\"/jKt1cBEY99awXmLeroZ9vtTe0ig3LQQdlFbTvu0Szc=\"}},\"hash\":\"WEHy3NpRBfRUCjwp126fIw/usFVAmefpavTjsv+9CN0=\",\"text\":\"most individuals consolidate and define the major elements of their identi-\\r\\nties. In other words, to recast the example that Smith finds exemplary,\\r\\nAngela’s Ashesfollows the emerging consciousness of a boy within the con-\\r\\ntext of the domestic sphere of family relations, and gradually widens the\\r\\nscope of that consciousness as McCourt enters the adult world. Indeed, the\\r\\nvery title of McCourt’s text—referring to his mother—is suggestive of the\\r\\ncritical determining influence of human connection upon identity\\r\\nconstruction. In my view, the problem lies in the fact that the canonically exemplary\\r\\ncoming-of-age narrative of the socially isolated and alienated individual is\\r\\nrarely successful. Huck Finn’s coming of age is arrested because finally he is\\r\\nunable to reconstitute the self in light of his experiences. At the end of the\\r\\nnarrative, Jim has joined his family, and Huck is left standing on the margins\\r\\nof society, attempting to persuade himself that a solitary life in the territories\\r\\nis preferable to the comforts and supports of human company. And Holden\\r\\nCaulfield, that other archetypal American literary male, has a mental break-\\r\\ndown because the prospect of being emotionally self-sufficient is simply too\\r\\npainful—the expectation that he do so creates an untenable inner conflict. The autonomous individual is profoundly alone and lonely. In contrast,\\r\\nwomen, who have long labored under the expectation that they mustcultivate\\r\\nrelationships to the exclusion of all else, have created a rite of passage in the\\r\\ncoming-of-age narrative that refuses the binary opposition and recognizes the\\r\\nmultivalent desires of the individual—for relationship andfor agency. In order to contextualize my argument, in the next chapter I provide a\\r\\nbrief overview of the significance of women’s narratives in the American fem-\\r\\ninist movement. In my view, the core issue at the heart of all feminist schol-\\r\\narship and activism is the struggle over female subjectivity, and so I have\\r\\nsituated my analysis in Western philosophical theories of the subject in order\\r\\nto trace the origins of current autobiographical practices. Women’s political\\r\\nactivism has been deeply informed by and responsive to the Enlightenment\\r\\nideal of selfhood, and consequently, autobiographical texts by women politi-\\r\\ncal activists, artists, and scholars have contributed significantly to dismantling\\r\\nthat ideal. In the third chapter, I examine the evolution of adolescence as a concept\\r\\nin American culture generally, and in American literature specifically. Recog-\\r\\nnized as a distinct phase of development only in the last century, adolescence\\r\\nand its specific features are now the focus of greatly detailed study in certain\\r\\nfields, most notably in psychology. Until very recently, most such studies\\r\\nhave used male models as the norm and, as a result, their findings are often\\r\\n18\\u0001From Girl to Woman\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"b28e5282-d618-4e48-b479-15effa69bc03":{"__data__":"{\"id_\":\"b28e5282-d618-4e48-b479-15effa69bc03\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"essay.pdf||AVro3p5zYiuJ0QXPJftCAZR8m/dK9j8Rn4N177fY6mU=\",\"metadata\":{},\"hash\":\"izfVOoHh1Ugg3+5mn8ZcvM+xsgS0nwcF0U3Wl6NzujE=\"}},\"hash\":\"x/c+ZBjq7IIxWEO63YoOGcUxj1VHo2miwW9CsUI61Gs=\",\"text\":\"of limited use in understanding female development. I also review an anthro-\\r\\npological perspective of adolescence, beginning with Margaret Mead’s influ-\\r\\nential study of Samoan culture. Although her work has become highly\\r\\ncontroversial, I borrow from her methods (albeit lightly) and that of more\\r\\nrecent anthropological theory to widen the lens in my analysis of literary\\r\\ntexts. Finally, I explore the idea of “coming of age” as it occurs in literature\\r\\nbeginning with the Bildungsroman, and the ways in which women writers\\r\\nhave altered the traditional genre to reflect diverse patterns of development. The aims of Bildungsroman and autobiography converge in the latter part of\\r\\nthe twentieth century, resulting in blurred genre distinctions and a radically\\r\\nrevised narrative structure. I offer my views of what coming of age means,\\r\\nand argue that the proliferation of its treatment in American letters provides\\r\\na useful indicator of the changing conceptions of identity. Next, I turn to Annie Dillard’s 1987 An American Childhoodand Anne\\r\\nMoody’s 1968 Coming of Age in Mississippi. While these texts are radically dif-\\r\\nferent in form and content, their authors from opposite ends of the social spec-\\r\\ntrum in terms of privilege and social standing, I have chosen to discuss them\\r\\nin tandem to highlight how those differences function in an autobiographical\\r\\ntext and indeed might serve to determine its form. As Sidonie Smith has writ-\\r\\nten, Dillard’s very title seems to invoke a universal subjectivity with the\\r\\nmodifier “American” attached to another general category, “childhood” (Sub-\\r\\njectivity131). Dillard makes a seemingly transparent assumption that she is\\r\\ndescribing a universal version of childhood, an observation borne out in the\\r\\ntext by numerous references to what “any” child feels or experiences, but also\\r\\ncontradicted by a richly specific textual self. Following Smith’s observation, I\\r\\nargue then that, in contrast, Moody’s title invokes a highly specified subjec-\\r\\ntivity, placing her text/life story in a particular locale, and thus creating specific\\r\\nexpectations in the reader. Unlike Dillard, who writes that she “slid into [her-\\r\\nself] perfectly fitted” (11), Moody slides into herself and finds the fit uncom-\\r\\nfortable and ill-sized. The pain and conflict that accompany her attempts to\\r\\ncreate an identity she is comfortable with are never fully resolved in her text,\\r\\nand she remains an unfinished subject to a greater degree than is evident in\\r\\ntraditional autobiographies. Indeed, the coming-of-age narrative is distin-\\r\\nguished in part by a provisional subjectivity forged in ideological conflicts and\\r\\nsubject to change as a direct result of subsequent experiences. Chapter 4 considers the problem of genre boundaries and truth in auto-\\r\\nbiography in my discussion of Mary McCarthy’s Memories of a Catholic Girl-\\r\\nhood(1955) and two texts by Zora Neale Hurston: Their Eyes Were Watching\\r\\nGod (1937) and Dust Tracks on a Road (1942). McCarthy answers anticipated\\r\\nIdentity and the Coming-of-Age Narrative\\u000119\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"ac8627d4-3f97-4f0c-acce-9fd6a56da1fe":{"__data__":"{\"id_\":\"ac8627d4-3f97-4f0c-acce-9fd6a56da1fe\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"essay.pdf||98yff55I1Orl0Nxh93nipnrNbr6WMtxKHXc1eOwZi/s=\",\"metadata\":{},\"hash\":\"q3M0Qn5wfsqoFLeoFqtowC3jGXGgkHx0kauwoIXsYR0=\"}},\"hash\":\"SjSlwLEb4sEp+E4izN26GAop3SAaQTI6Z7OuD/c3d7I=\",\"text\":\"questions about the reliability of memory through the use of italicized pas-\\r\\nsages in which she deconstructs her narrative, and explicitly admits to using\\r\\nthe conventions of fiction to fill in the gaps. In contrast, Hurston’s autobiog-\\r\\nraphy is, according to Robert Hemenway, full of unacknowledged lies, while\\r\\nTheir Eyes Were Watching Godis widely understood to be an “autobiographical”\\r\\nnovel that is revealing of aspects of Hurston’s life about which she remained\\r\\nsilent in Dust Tracks. These texts raise complex questions about the writer’s\\r\\nobligation to be truthful, and the complementary problem of how, as readers,\\r\\nwe ought to address the unavoidable gaps and silences (in Macherey’s terms)\\r\\nof any autobiography. In chapter 6, I discuss the conflicts and issues surrounding the hybridiza-\\r\\ntion of identities—national, ethnic, religious—that are the hallmark of many\\r\\nimmigrant coming-of-age autobiographies. The tension and confusion that\\r\\naccompany a child’s negotiation of her parents’ identity with an “American”\\r\\nidentity results in a particular and specific journey through adolescence that\\r\\ndiffers substantially from that of a child whose parents’ national/ethnic iden-\\r\\ntity is not in flux. The autobiographical text of a first-generation American\\r\\ngirl is deeply informed by the conflicts between two cultures in addition to\\r\\nthe contradictions already inherent in a single culture. In addition to the\\r\\nusual host of identity issues that beset a girl as she comes of age, she must\\r\\nalso negotiate the difficult terrain of ethnic subjectivity. Textual representa-\\r\\ntions of these issues are as varied as the number of combinations of cultures\\r\\npossible, and while I do not suggest that the texts I have chosen for this dis-\\r\\ncussion are necessarily representative of all such autobiographies, they do\\r\\nillustrate the specific difficulties of identity formation in the context of major\\r\\ncultural differences. Maxine Hong Kingston’s The Woman Warrior (1975)\\r\\nbegins with her mother saying, “You must not tell anyone what I am about to\\r\\ntell you,” thus setting the stage for Kingston’s difficult lessons in the secrecy\\r\\nand silence surrounding women’s lives that is typical of Chinese culture in\\r\\nopposition to the relative openness of American culture. Kingston’s text jux-\\r\\ntaposes myth and realistic narrative as a formal device that highlights the\\r\\nconflicting discourses and ideologies she negotiates in the process of con-\\r\\nstructing a workable identity. Kate Simon’s 1982 Bronx Primitive also exhibits\\r\\nthe tensions inherent in immigrant subjectivity, but where Kingston is some-\\r\\ntimes paralyzed by the conflicts between Chinese and American norms,\\r\\nSimon quickly learns to take advantage of the contradictions in order to do as\\r\\nshe pleases, even though she is initially confused by the contradictory mes-\\r\\nsages of her parents and the wider society. As she grows older, she flatly\\r\\nrejects much of the Old World interpellation of Jewish female subjectivity\\r\\n20\\u0001From Girl to Woman\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"fac9fb71-abdd-40b5-99b6-ab1d255fc8a7":{"__data__":"{\"id_\":\"fac9fb71-abdd-40b5-99b6-ab1d255fc8a7\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"essay.pdf||3RkIAqyXKUOIVxbg53LAYPx1OAlclGPJWcV0tmfBPF8=\",\"metadata\":{},\"hash\":\"rss+Cpt0MvClJTdB3EsaYcOS5CArTbZyJhZyI0ajO80=\"}},\"hash\":\"Pqdg2Q72YpnO4O+0DwvBd7ojdD4CtztYgBb/dnXCERw=\",\"text\":\"offered by her parents. Furthermore, Bronx Primitive provides a reasonably\\r\\nexplicit model of Williams’s model of imagining agency, and the ways in\\r\\nwhich naming the contradictions in ideology allows for resistance to the\\r\\nnorm. Arguing that the construction of identity varies according to the particu-\\r\\nlar historical moment of the events narrated (but perhaps more importantly,\\r\\nthe moment of writing), I interrogate the construction of female identity in\\r\\nthe coming-of-age narrative. The institutions, practices, and prevailing hege-\\r\\nmony at the time of writing exert certain limits on what the writer can and\\r\\ncannot say, and on what discourses of womanhood will be tolerated. While I\\r\\nwould not argue that the decisions a girl makes in adolescence will determine\\r\\nthe woman she will become in any permanent sense, the process of coming\\r\\nof age does involve examining one’s choices and deciding—consciously or\\r\\nnot—what direction one’s life will take. Alternatively, she might watch in\\r\\ndismay as her horizons shrink, her choices become limited, and her life is\\r\\nseemingly mapped out for her. Nonetheless, it is in adolescence that she\\r\\nlearns, sometimes forcefully, exactly what forms of subjectivity and narratives\\r\\nare available to her and which forms will cause her to be marginalized. Of\\r\\ncourse, assenting to one of the socially sanctioned subjectivities available to\\r\\nher will still not guarantee social acceptance. If Anne Moody had accepted\\r\\nthe hegemonic view of black womanhood, for example, it might have\\r\\nresulted in her personal safety but not widespread social approval and accept-\\r\\nance. It is in adolescence that the child becomes mother to the woman. Throughout the study, I’m making a primarily positive argument about\\r\\nthe apparent increase in a woman’s range of acceptable subjectivities, and\\r\\ntheir ability to resist social “limits and pressures” to create new discourses of\\r\\nwomanhood. I do not deny the genuine pain and conflict that are the conse-\\r\\nquences of rejecting received notions of womanhood, but ultimately, this\\r\\ndynamic makes alternative forms of subjectivity possible—even necessary. Identity and the Coming-of-Age Narrative\\u000121\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"9dafdb42-4539-487d-b0dd-8ce49f8761a4":{"__data__":"{\"id_\":\"9dafdb42-4539-487d-b0dd-8ce49f8761a4\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"price_matching (2).docx||shgnz\",\"metadata\":{},\"hash\":\"HluvWOwR4avW6JX/czwMKwLEyVT1n0Z+vLAN/Dwt8Fg=\"}},\"hash\":\"fdRkmmXqZfSb9q6O0+ln/zbDiYlv61YoT3rSdz9lWsM=\",\"text\":\"Price Matching\\n\\nHow the Low Price Guarantee works for online purchases: Our Price Match Guarantee includes the price of the item(s) plus shipping cost. Price Match items must be available from competitor to ship to customer’s location. We will only honor requests submitted directly from the person who made the purchase. Shop Now\\n\\nHow the Low Price Guarantee works for in-store pre-purchases: If you find a current lower price on an identical, in-stock item from any other retailer, we will match the price. Just bring the ad, printout or photo with you to the register for validation. (This may involve the associate contacting the competitor). Find Your Local Store\\n\\nWhat's Not Included? Seasonal, discontinued, obsolete, clearance, distressed or going-out-of-business sale merchandise\\n\\nCustom or special order products - meaning customized or configured products ( e.g., custom ordered blinds or carpet) from a competitor\\n\\nVolume or wholesale discount pricing offered by competitors\\n\\nItems sold via a third-party or discount site - meaning any products that are not sold directly by a competing retailer\\n\\nOpen-box merchandise as well as reconditioned, refurbished or previously discounted products\\n\\nCompetitor pricing for professional services, including labor and installation costs\\n\\nSales tax is excluded\\n\\nRebates, free offers, or specific one-time-only promotions, including seasonal promotions (such as Spring Black Friday, Black Friday and various holiday promotions), special events and/or bundle offers. Advertised pricing from a competitor based on typographical error or misprint\\n\\nPricing from membership-based retail wholesalers \\n\\nCompetitor bid and auctions site pricing\\n\\nPrice matching on financed offers or credit terms\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"85bdc115-15b9-4d13-a4bb-7173d7061fb5":{"__data__":"{\"id_\":\"85bdc115-15b9-4d13-a4bb-7173d7061fb5\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"www_homedepot_com_c_customer_service.txt||n7r6q\",\"metadata\":{},\"hash\":\"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs=\"},\"NEXT\":{\"nodeId\":\"bfe92bca-1fdb-44c5-92ad-c8bfd3eccd2a\",\"metadata\":{},\"hash\":\"i9AVusQwwsnVFevBJPx+w3dvm3PloTc+ZDBTOO2R1+8=\"}},\"hash\":\"icjMNTbB8GynO6TdniFHTNgETrodsK03wJTIDU4H+Do=\",\"text\":\"Shop All Savings Appliances Bath & Faucets Blinds & Window Treatment Building Materials Lumber & Composites Cleaning Decor & Furniture Lighting & Ceiling Fans Doors & Windows Electrical Flooring & Area Rugs Hardware Heating & Cooling Kitchen & Kitchenware Lawn & Garden Outdoor Living & Patio Paint Plumbing Seasonal Decor Smart Home Storage & Organization Tools Automotive Furniture Gift Cards Credit Cards View All Home Services Tool Rental Benefits & Services For the Pro Project Calculators View All DIY Projects Local Ad Store Finder Site Map#1 Home Improvement RetailerSelect store......Shop AllServicesDIYMeCart\\n    HomeHelp and Customer Service CenterHelp and Customer Service CenterExplore Help TopicsOnline Order SupportIn-Store Purchase SupportCredit Customer ServiceExplore Help TopicsORDERS & PURCHASESTrack Your OrderOrder CancellationReturn PolicyChange Appliance Delivery DateView Order HistoryView In-Store ReceiptsCreate & Manage AccountPAYMENTS & PRICINGPay Credit CardPrice MatchingGift Cards & Store CreditsTax ExemptionsSHIPPING & STORE PICKUPShipping & Delivery FAQFree Shipping OptionsBuy Online & Pick Up in StoreBuy Online & Ship to StoreBuy Online & Deliver from StorePRODUCT INFORMATIONProtection PlansProduct RecallsProduct RebatesSeeds ProgramSERVICESInstallation ServicesTools and Truck RentalMoving ServicesPro ServicesBUSINESS RESPONSIBILITYHome Depot FoundationBuilding Business ResponsibilityHonoring Our VeteransCONNECTIONSInvestor RelationsSuppliers & ProvidersAffiliate ProgramCareersLEGALTerms of UsePrivacy & Security StatementManage My Marketing PreferencesCalifornia Privacy Rights & ReportsPrivacy & Security CenterCheck Order StatusFind Store Locations & HoursView Return PolicyManage Credit AccountsOnline Order SupportText with Online Customer Support\\n\\nTo reach us for support, text SUPPORT to 38698 and start a conversation with our online customer care team. Message and data rates may apply. Message frequency varies. Reply HELP for help and STOP to stop. Please see our Terms of Use and Privacy & Security Statement.Call us\\n\\nHomedepot.com\\n\\nMajor Appliances Support: 1-800-455-3869\\n\\nHomedepot.com Online Order Support: 1-800-430-3376\\n\\nView Our DirectoryIn-Store Purchase SupportCall us\\n\\nPrimary Customer Service\\n\\n1-800-HOME-DEPOT (466-3337)\\nAdditional Resources \\n\\nWant to instantly check the status of an order? Track Your Order\\n\\nWant to email us? Please be aware that responses via email may currently take up to 7 days. For immediate assistance, please contact our representatives via phone at 1-800-HOME-DEPOT (466-3337). To continue, click here to send us an email.Consumer & Commercial Credit Customer ServiceConsumer Credit Card Account\\n\\n1-800-677-0232 or TTY:711Commercial Revolving Charge Card\\n\\n1-800-685-6691 or TTY:711Commercial Credit Card Account\\n\\n1-800-395-7363 or TTY:711Whether you want to check your online order status, manage your Home Depot credit card or check your Home Depot gift card balance, our online Help Center can help answer any questions you may have. You can also use our site to access more information about the Home Depot return policy. For immediate assistance, contact our Customer Service department at 1-800-HOME DEPOT (466-3337) to speak with one of our representatives. Use our Store Finder to locate your nearest Home Depot, where you can get help from our friendly associates and even attend in-store workshops. Download Our AppHow doers get more done™Need Help? Please call us at: 1-800-HOME-DEPOT(1-800-466-3337)Special Financing Available everyday*Pay & Manage Your CardCredit OffersGet $5 off when you sign up for emails with savings and tips.GOOur Other SitesThe Home Depot CanadaThe Home Depot MéxicoPro ReferralShop Our BrandsHow can we help?Call 1-800-466-3337|Text 38698© 2000-2024 Home Depot. All Rights Reserved. Use of this site is subject to certain Terms Of Use.Local store prices may vary from those displayed.\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"},"bfe92bca-1fdb-44c5-92ad-c8bfd3eccd2a":{"__data__":"{\"id_\":\"bfe92bca-1fdb-44c5-92ad-c8bfd3eccd2a\",\"metadata\":{},\"excludedEmbedMetadataKeys\":[],\"excludedLlmMetadataKeys\":[],\"relationships\":{\"SOURCE\":{\"nodeId\":\"www_homedepot_com_c_customer_service.txt||n7r6q\",\"metadata\":{},\"hash\":\"sa4b9doRz6FEUcGZ9ttIcluS0UUqqu/D1sHWsBSWsbs=\"},\"PREVIOUS\":{\"nodeId\":\"85bdc115-15b9-4d13-a4bb-7173d7061fb5\",\"metadata\":{},\"hash\":\"icjMNTbB8GynO6TdniFHTNgETrodsK03wJTIDU4H+Do=\"}},\"hash\":\"i9AVusQwwsnVFevBJPx+w3dvm3PloTc+ZDBTOO2R1+8=\",\"text\":\"Use of this site is subject to certain Terms Of Use.Local store prices may vary from those displayed. Products shown as available are normally stocked but inventory levels cannot be guaranteed.For screen reader problems with this website, please call 1-800-430-3376 or text 38698 (standard carrier rates apply to texts).Stores | ©2000-2024 Home Depot | Privacy & Security Statement | Terms | My Preference Center | California Privacy Rights & Report | Limit the Use of My Sensitive Personal Information | Do Not Sell or Share My Personal Information | California Supply Chain ActSite MapStore DirectoryProvide Feedback\\n    \\n    \\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n\\n\\n\\n        \\n    \\n\\n\\n\\n\\n\\n    \\n\\n    \\n        \\n    \\n    \\n  \\n<img src=\\\"https://www.homedepot.com/akam/13/pixel_50dc5c8?a=dD1hYTQ2ODVlOGYzMmFmYzY2YTY0ZTE0MDk3ZjU5YWMyOWJmOGE4ZDI3JmpzPW9mZg==\\\" style=\\\"visibility: hidden; position: absolute; left: -999px; top: -999px;\\\" />\",\"textTemplate\":\"\",\"metadataSeparator\":\"\\n\",\"type\":\"TEXT\"}","__type__":"TEXT"}},"docstore/ref_doc_info":{"2401.05391.pdf||FPgfDWs0WtFnGL7lRuconY+HIB0RmbLjVaE5+fEX4Vo=":{"nodeIds":["67999390-1e05-40e0-91c1-3c4562891a6f"],"extraInfo":{}},"2401.05391.pdf||SoBeMvGB1QubAbrhzjQTWp4hsrXKG4nwbTrR0lVEZaA=":{"nodeIds":["c2c49fac-2af2-4f46-9532-31d7e0b237c5"],"extraInfo":{}},"2401.05391.pdf||CkgjS4b8v+90JCWIwF0XjwhzUgTy4MwSwMgHUMAdLi8=":{"nodeIds":["44d6fc32-366c-41fe-a557-c9f6fba4592e"],"extraInfo":{}},"2401.05391.pdf||PHMAujqg3eZ1PkPWK5PE7YXBgvpkcXYWt5aALI3o29s=":{"nodeIds":["56dbac1d-3be7-40cc-b8c4-14279a1ef278"],"extraInfo":{}},"2401.05391.pdf||0XBRAARuuqBKBmmcn4NR6XEfLtHhFWNuIN8pMZ1Um4I=":{"nodeIds":["8cf778f9-1718-4805-8ba8-d805ac2ca2f9"],"extraInfo":{}},"2401.05391.pdf||fhS37/t0afYTMesAWz1Rzdi5ZxlQH3pn1h2bx25uMso=":{"nodeIds":["61741d4c-5a5d-4532-8a47-7840b5f875d7"],"extraInfo":{}},"2401.05391.pdf||68yz/qH1CKcYhL0Gm7Lnh1vl7yLjRfi5tkA+I03QvYU=":{"nodeIds":["07fc4094-f845-422a-883c-1f218646e43e"],"extraInfo":{}},"2401.05391.pdf||tnNg4cV+xlEYfl/0P/oKvIptKfdU+5VeXX/aLGeYbt8=":{"nodeIds":["ab43e7fc-73bf-4a9e-88ea-b5bdbcd67879"],"extraInfo":{}},"2401.05391.pdf||cuua4OU7/eRcqfCw+feXEHT7a3fxuFdex8NeWk/RVLI=":{"nodeIds":["75d1f433-1cdc-4b7f-a696-1e8c07ece9f7"],"extraInfo":{}},"2401.05391.pdf||ahinDG5CS6JNHc0LykVHln7mmqxlEAJc4oBU80Y6RUs=":{"nodeIds":["a7b0b44d-b90b-4c8e-8b3e-f73aa39010d7"],"extraInfo":{}},"2401.05391.pdf||kdLwvQdHeAvOXGhqdT/Jhep7xdgouLZHk1xSV6y76Lo=":{"nodeIds":["a2d69a4b-410e-41e7-803f-3cae26b021c2"],"extraInfo":{}},"2401.05391.pdf||ZgqiqrnqxytWGHAvVDkRtCJAufj7OM6Dq65qoWqaiM0=":{"nodeIds":["cc0cc649-f4b0-4459-ba43-282aa91db279"],"extraInfo":{}},"2401.05391.pdf||N96CZB5UOzfpjjxBj/pX8sh6diAYTORnU5FhZ98VOug=":{"nodeIds":["291847c9-5083-4aed-9446-baafe0c7935a"],"extraInfo":{}},"2401.05391.pdf||PN27Am8tNMOXdDexl0RPsb7mp3lm16abkKdNgQRC11g=":{"nodeIds":["1db7a1aa-9845-44c2-bdf7-7725be1ef807","4ff7662b-9440-4d99-95fb-f3fcb7839d30"],"extraInfo":{}},"2401.05391.pdf||AFYEcw7idVLwi4JQtpVAhUEzH8E+vAqkNqT0u+IjbQ0=":{"nodeIds":["2c367b1d-3edd-4de6-bdab-c56d0e114216","6ff95514-ceb9-4d8e-8b45-4bf7abc14b83"],"extraInfo":{}},"corporate_homedepot_com_.txt||14kna":{"nodeIds":["99f6f9a7-46f0-4c3a-a80d-7480f937b103","78730d49-754b-4cc7-96a4-fa3192cb694c"],"extraInfo":{}},"essay.pdf||4YcMzZLR/BL09BruoFDGPRA2vb8xcRXtFvZtKsIVjDA=":{"nodeIds":["81155eae-fc19-4ab0-bd03-531194a5ecde"],"extraInfo":{}},"essay.pdf||mg591T7N8x8xBVVfOu26Q6GDKhdlU032K6IvJVBAZvg=":{"nodeIds":["6c5773ed-a2cd-4293-ae91-92813a73a125"],"extraInfo":{}},"essay.pdf||j+D09K2bvXXJsI4Lyr2pTE7NhdSvCTr5oiijxwDd4GQ=":{"nodeIds":["1497e329-c523-4308-b5cd-c2f04631cfba"],"extraInfo":{}},"essay.pdf||8uO5RtoMt6K1rDdWpLDf1J3FyomO4y6vYAr8W8gwAcM=":{"nodeIds":["67597179-519d-495c-9849-f3777fcff97f"],"extraInfo":{}},"essay.pdf||I8IRYbaHIQZkDh0aDwFb3zKPwJ8w9xrzHIMvept79R8=":{"nodeIds":["ae1e3cdc-9780-4e35-bc5e-289c158a5026"],"extraInfo":{}},"essay.pdf||+dKnlkpfisOY1lBGC3O0heom5/B3/34cC7OEUM1p9NM=":{"nodeIds":["1322b5d4-acd3-4865-a45a-c8074862f3dd"],"extraInfo":{}},"essay.pdf||Z/fmghpU+nLad+Ro9mun6AQ4k0P8egZ4pgIcq6TYgPE=":{"nodeIds":["16150cba-cff9-49c2-9ba0-3390bd9ce96f"],"extraInfo":{}},"essay.pdf||QukymsQibVbJyCoYH6SuN55rjl/sGCTwHIZfBIKrtjs=":{"nodeIds":["a0aa49aa-c043-4c19-88da-792eda43c5a7"],"extraInfo":{}},"essay.pdf||/QanAuJ+7y0ZjTptqVeO8pbeXFT2gpW6Ob/LOoZSvO0=":{"nodeIds":["f8d24ac5-62a9-4a08-b325-d67ba54fa5c2"],"extraInfo":{}},"essay.pdf||pEUEspFmpXDtZzpPZCWHQYbgoI0q+cJT4La12LHuhzI=":{"nodeIds":["b6d30409-8c58-4d0f-a767-dcb90d312c83"],"extraInfo":{}},"essay.pdf||FeyRGTbuLhwCOCYxXZhHpwd6nuub/T10TFTNHIDJin8=":{"nodeIds":["44ebb065-39b9-4308-ada5-32eacc0e96fe"],"extraInfo":{}},"essay.pdf||/jkjEGY6UxdUsTMGTf7YglD20FCJfpQanlKHMiuy6cY=":{"nodeIds":["0e1f51a7-8e39-41be-97ee-3a5ac4097a21"],"extraInfo":{}},"essay.pdf||/y2IZVS31txDAjm3H617Sdpz14hB8T72qfsienrU4ek=":{"nodeIds":["47c37987-d689-4f7d-af71-50812fd50231"],"extraInfo":{}},"essay.pdf||QcklhPs33xTIMyeacu01w9LS+l+u2+HUTaN6awrz+BQ=":{"nodeIds":["e68e1428-f18e-4d46-a272-ea7a1db0856f"],"extraInfo":{}},"essay.pdf||wn/J2PyaEjVQX7jkicfo6si2JGGn+FLGyrcwaN9ORiA=":{"nodeIds":["aedfc1e7-6307-47af-8373-c9bc476e7ff5"],"extraInfo":{}},"essay.pdf||mTqQb2yceI3Otc7/I2+229A97vK67/xknKuvavo8p88=":{"nodeIds":["28b4a213-6372-4af3-a71e-5704193001c6"],"extraInfo":{}},"essay.pdf||whEERXPHbiOjJRpOIBm8HPTkHYYZJ/cqZYpoe/qrhe8=":{"nodeIds":["cd677ed3-a582-4887-baaf-c067a62efcef"],"extraInfo":{}},"essay.pdf||Ft6v52B2K3Z3zf0d+7UenLQpXXF6jTjkl2jqGbOWhlc=":{"nodeIds":["f19f7184-9095-4b01-a643-2dee85348df0"],"extraInfo":{}},"essay.pdf||AVro3p5zYiuJ0QXPJftCAZR8m/dK9j8Rn4N177fY6mU=":{"nodeIds":["b28e5282-d618-4e48-b479-15effa69bc03"],"extraInfo":{}},"essay.pdf||98yff55I1Orl0Nxh93nipnrNbr6WMtxKHXc1eOwZi/s=":{"nodeIds":["ac8627d4-3f97-4f0c-acce-9fd6a56da1fe"],"extraInfo":{}},"essay.pdf||3RkIAqyXKUOIVxbg53LAYPx1OAlclGPJWcV0tmfBPF8=":{"nodeIds":["fac9fb71-abdd-40b5-99b6-ab1d255fc8a7"],"extraInfo":{}},"price_matching (2).docx||shgnz":{"nodeIds":["9dafdb42-4539-487d-b0dd-8ce49f8761a4"],"extraInfo":{}},"www_homedepot_com_c_customer_service.txt||n7r6q":{"nodeIds":["85bdc115-15b9-4d13-a4bb-7173d7061fb5","bfe92bca-1fdb-44c5-92ad-c8bfd3eccd2a"],"extraInfo":{}}}}